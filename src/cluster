#!/bin/sh

#
# Handle installer tasks
#

check_cluster_exists_to_create() {
  if [[ -d ${CLUSTER_HOME} ]]; then
    echo "Cluster directory [${CLUSTER_HOME}] already exists.";
    if [[ ${OPT_FORCE:-} == false ]]; then
        echo "--force flag was not found, exiting...";
        exit 1
    fi
  fi
}

# Check cluster dir exists to delete
check_cluster_exists_to_destroy() {
  if [[ ! -d ${CLUSTER_HOME} ]]; then
    echo "Cluster directory [${CLUSTER_HOME}] does not exists.";
    exit 1
  fi
}

check_dependencies_to_create() {
  if [[ ! -d ${CLUSTER_HOME} ]]; then
    mkdir -p ${CLUSTER_HOME}
  fi
}

backup_cluster_installdir() {
  tar cfJ "${CLUSTER_HOME}-$(date +%Y%m%d%H%M%S).tar.xz" ${CLUSTER_HOME}
}

destroy_cluster() {
  local bin_installer=$(yq -r .installer_path ${CFG_RENDERED})

  ${bin_installer} --dir ${CLUSTER_HOME} destroy cluster
}

#
#
#
render_runner_config() {

  local platform_name="$(yq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.platform ${GLOBAL_CONFIG})"

  # platform specifics
  if [[ "${platform_name}" == "aws" ]]; then
    vol_key="rootVolume"
  elif [[ "${platform_name}" == "azure" ]]; then
    vol_key="osDisk"
  else
    echo "Platform not found to render config, exiting..."
    exit 1;
  fi

  # check baseDomain
  local baseDomain_cluster="$(yq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.basedomain ${GLOBAL_CONFIG} || true)"
  local baseDomain_plat="$(yq -r .defaults.${platform_name}.baseDomain ${GLOBAL_CONFIG})"
  if [[ -z "${baseDomain_cluster}" ]] || [[ "${baseDomain_cluster}" == "null" ]]; then
    baseDomain="${baseDomain_plat}"
  else
    baseDomain="${baseDomain_cluster}"
  fi

  # instance/vm type: control plane (cpl) and compute (comp)
  local vm_cpl_type_cluster="$(yq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.controlPlane.type ${GLOBAL_CONFIG} || true)"
  local vm_cpl_type_plat="$(yq -r .defaults.${platform_name}.controlPlane.type ${GLOBAL_CONFIG} )"
  if [[ -z "${vm_cpl_type_cluster}" ]] || [[ "${vm_cpl_type_cluster}" == "null" ]]; then
    vm_type_cpl="${vm_cpl_type_plat}"
  else
    vm_type_cpl="${vm_cpl_type_cluster}"
  fi

  local vm_comp_type_cluster="$(yq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.compute.type ${GLOBAL_CONFIG} || true)"
  local vm_comp_type_plat="$(yq -r .defaults.${platform_name}.compute.type ${GLOBAL_CONFIG} )"
  if [[ -z "${vm_comp_type_cluster}" ]] || [[ "${vm_comp_type_cluster}" == "null" ]]; then
    vm_type_comp="${vm_comp_type_plat}"
  else
    vm_type_comp="${vm_comp_type_cluster}"
  fi

  # Vol types
  local vol_cpl_type_cluster="$(yq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.vol_type ${GLOBAL_CONFIG} || true)"
  local vol_cpl_type_plat="$(yq -r .defaults.${platform_name}.vol_type ${GLOBAL_CONFIG} )"
  if [[ -z "${vm_cpl_type_cluster}" ]] || [[ "${vm_cpl_type_cluster}" == "null" ]]; then
    vol_type_cpl="${vol_cpl_type_plat}"
    vol_size_cpl="$(yq -r .defaults.${platform_name}.vol_size ${GLOBAL_CONFIG} )"
  else
    vol_type_cpl="${vol_cpl_type_cluster}"
    vol_size_cpl="$(yq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.vol_size ${GLOBAL_CONFIG} )"
  fi

  # TODO: customize disk for cluster roles (master, worker, etc)
  # local vol_comp_type_cluster="$(yq . ${GLOBAL_CONFIG} |jq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.vol_type || true)"
  # local vol_comp_type_plat="$(yq . ${GLOBAL_CONFIG} |jq -r .defaults.${platform}.vol_type )"
  # if [[ -z "${vm_comp_type_cluster}" ]] || [[ "${vm_comp_type_cluster}" == "null" ]]; then
  #   vol_type_comp="${vm_comp_type_plat}"
  #   vol_size_cpl="$(yq . ${GLOBAL_CONFIG} |jq -r .defaults.${platform}.vol_size )"
  # else
  #   vol_type_comp="${vm_comp_type_cluster}"
  #   vol_size_comp="$(yq . ${GLOBAL_CONFIG} |jq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.vol_size)"
  # fi

  # Replica count
  local rpl_count_cpl="$(yq -r .defaults.${platform_name}.controlPlane.replicas ${GLOBAL_CONFIG})"
  local rpl_count_comp="$(yq -r .defaults.${platform_name}.controlPlane.replicas ${GLOBAL_CONFIG} )"
  if [[ -z "${rpl_count_cpl}" ]] || [[ "${rpl_count_cpl}" == "null" ]]; then
    rpl_count_cpl=3
  fi
  if [[ -z "${rpl_count_comp}" ]] || [[ "${rpl_count_comp}" == "null" ]]; then
    rpl_count_comp=3
  fi

  # Arch
  local nodes_arch="$(yq -r .defaults.${platform_name}.architecture ${GLOBAL_CONFIG} )"
  if [[ -z "${nodes_arch}" ]] || [[ "${nodes_arch}" == "null" ]]; then
    nodes_arch="amd64"
  fi

  # installer path
  local bin_installer="$(yq -r .cluster_profiles.${OPT_CLUSTER_PROFILE}.installer ${GLOBAL_CONFIG} || true)"
  if [[ -z "${bin_installer}" ]] || [[ "${bin_installer}" == "null" ]]; then
    echo "ERROR: {installer} not found on Cluster Profile section"
    exit 1
  fi
  local bin_installer="${WORKDIR}/bin/${bin_installer}"
  if [[ ! -x ${bin_installer} ]]; then
    echo "ERROR: installer binary not found on path [${bin_installer}]"
    exit 1
  fi

  cat << EOF > ${CFG_RENDERED}
cluster_name: ${OPT_CLUSTER_NAME}
baseDomain: ${baseDomain}

# Platform specifics
platform: ${platform_name}
platform_options: $(yq -r .defaults.${platform_name}.platform ${GLOBAL_CONFIG} )
vol_key: ${vol_key}

nodes_architecture: ${nodes_arch}

controlPlane_replicas_count: ${rpl_count_cpl}
controlPlane_vm_type: ${vm_type_cpl}
controlPlane_vol_type: ${vol_type_cpl}
controlPlane_vol_size: ${vol_size_cpl}

compute_replicas_count: ${rpl_count_comp}
compute_vm_type: ${vm_type_comp}
compute_vol_type: ${vol_type_cpl}
compute_vol_size: ${vol_size_cpl}

# Network
net_type: $(yq -r .defaults.network.type ${GLOBAL_CONFIG} )
net_cluster_cidr: $(yq -r .defaults.network.cluster_cidr ${GLOBAL_CONFIG} )
net_cluster_prefix: $(yq -r .defaults.network.cluster_prefix ${GLOBAL_CONFIG} )
net_machine_cidr: $(yq -r .defaults.network.machine_cidr ${GLOBAL_CONFIG} )
net_service_cidr: $(yq -r .defaults.network.service_cidr ${GLOBAL_CONFIG} )

# Paths / bins
installer_path: ${bin_installer}
EOF
}

create_install_config() {

  # Check dependencies
  if [[ -z ${PULL_SECRET} ]]; then
    echo "Environment var is not defined: PULL_SECRET. Exiting...";
    exit 1;
  fi

  if [[ -z ${SSH_PUB_KEYS} ]]; then
    echo "Environment var is not defined: SSH_PUB_KEYS. Exiting...";
    exit 1;
  fi

  # vars to render config
  local icfg_path_base="${CLUSTER_HOME}_install-config.yaml"
  local icfg_path_home="${CLUSTER_HOME}/install-config.yaml"

  # render config
  cat << EOF > ${icfg_path_base}
apiVersion: v1
baseDomain: $(yq -r .baseDomain ${CFG_RENDERED})
compute:
- architecture: $(yq -r .nodes_architecture ${CFG_RENDERED})
  hyperthreading: Enabled
  name: worker
  platform:
    $(yq -r .platform ${CFG_RENDERED}):
      type: $(yq -r .compute_vm_type ${CFG_RENDERED})
      $(yq -r .vol_key ${CFG_RENDERED}):
        size: $(yq -r .compute_vol_size ${CFG_RENDERED})
        type: $(yq -r .compute_vol_type ${CFG_RENDERED})
  replicas: $(yq -r .compute_replicas_count ${CFG_RENDERED})
controlPlane:
  architecture: $(yq -r .nodes_architecture ${CFG_RENDERED})
  hyperthreading: Enabled
  name: master
  platform:
    $(yq -r .platform ${CFG_RENDERED}):
      type: $(yq -r .controlPlane_vm_type ${CFG_RENDERED})
      $(yq -r .vol_key ${CFG_RENDERED}):
        size: $(yq -r .controlPlane_vol_size ${CFG_RENDERED})
        type: $(yq -r .controlPlane_vol_type ${CFG_RENDERED})
  replicas: $(yq -r .controlPlane_replicas_count ${CFG_RENDERED})
metadata:
  name: $(yq -r .cluster_name ${CFG_RENDERED})
networking:
  clusterNetwork:
  - cidr: $(yq -r '.net_cluster_cidr' ${CFG_RENDERED})
    hostPrefix: $(yq -r '.net_cluster_prefix' ${CFG_RENDERED})
  machineNetwork:
  - cidr: $(yq -r '.net_machine_cidr' ${CFG_RENDERED})
  networkType: $(yq -r '.net_type' ${CFG_RENDERED})
  serviceNetwork:
  - $(yq -r '.net_service_cidr' ${CFG_RENDERED})
platform:
  $(yq -r .platform ${CFG_RENDERED}): $(yq -r .platform_options ${CFG_RENDERED})
publish: $(yq -r .publish ${CFG_RENDERED})
pullSecret: '${PULL_SECRET}'
sshKey: |
  ${SSH_PUB_KEYS}

EOF
  cp ${icfg_path_base} ${icfg_path_home}
}

#
# Manifests Patchs
#

# Patch Machine Manifest for AWS to add two block devices
patch_machine_aws_2xDevBlocks() {

  local tmpdir=$(mktemp)
  rm -rf ${tmpdir}
  mkdir -p ${tmpdir}

  for machine_manifest in $(ls ${CLUSTER_HOME}/openshift/99_openshift-cluster-api_master-machines-*.yaml); do 
    
    local machine_file=$(basename ${machine_manifest})
    
    # extract header (pre blockDevices)
    sed '1,/blockDevices/!d' ${machine_manifest} > ${tmpdir}/${machine_file}_00

    # duplicate device list block
    cat << EOF > ${tmpdir}/${machine_file}_01-devs
      - ebs:
          encrypted: true
          iops: 0
          kmsKey:
            arn: ""
          volumeType: $(yq -r .controlPlane_vol_type ${CFG_RENDERED} )
          volumeSize: $(yq -r .controlPlane_vol_size ${CFG_RENDERED} )
      - deviceName: /dev/xvdb
        ebs:
          encrypted: true
          iops: 0
          kmsKey:
            arn: ""
          volumeType: $(yq -r .controlPlane_vol_type ${CFG_RENDERED} )
          volumeSize: $(yq -r .controlPlane_vol_size ${CFG_RENDERED} )

EOF
    # extract footer (post blockDevices
    grep credentialsSecret -A 999 ${machine_manifest} > ${tmpdir}/${machine_file}_03

    # join files
    cat ${tmpdir}/${machine_file}_* > ${tmpdir}/${machine_file}

    echo "Aggregated file created on ${tmpdir}/${machine_file} , overwriting to ${machine_manifest}"
    cp ${tmpdir}/${machine_file} ${machine_manifest}
  done
  rm -rf ${tmpdir}
}

patch_machineConfig_aws_mount_etcd() {

  # https://docs.fedoraproject.org/en-US/fedora-coreos/storage/
  # https://coreos.github.io/butane/examples/#mirrored-boot-disk
  
  cat << EOF > ${CLUSTER_HOME}/openshift/99_openshift-machineconfig_00-master-etcd.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 00-master-etcd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      disks:
      - device: /dev/nvme1n1
        wipe_table: true
        partitions:
        - size_mib: 0
          label: etcd
      filesystems:
        - path: /var/lib/etcd
          device: /dev/disk/by-partlabel/etcd
          format: xfs
          wipe_filesystem: true
    systemd:
      units:
        - name: var-lib-etcd.mount
          enabled: true
          contents: |
            [Unit]
            Before=local-fs.target
            [Mount]
            Where=/var/lib/etcd
            What=/dev/disk/by-partlabel/etcd
            [Install]
            WantedBy=local-fs.target
EOF
}

create_manifests() {
  local bin_installer=$(yq -r .installer_path ${CFG_RENDERED})

  rm -rf ${CLUSTER_HOME}/openshift ${CLUSTER_HOME}/manifests || true
  ${bin_installer} --dir ${CLUSTER_HOME} create manifests
}

create_patchs_when_create() {
  for patch in $(yq -r ".cluster_profiles.${OPT_CLUSTER_PROFILE}.manifest_patchs[]?" ${GLOBAL_CONFIG}); do
    echo "Looking for patch [${patch}]"
    ${patch}
  done
}

create_cluster() {
  local bin_installer=$(yq -r .installer_path ${CFG_RENDERED})

  ${bin_installer} --dir ${CLUSTER_HOME} create cluster
}

#
# Install Config
#

create_kubeconfig() {
    local auth_dir=$(dirname ${CLUSTER_KUBECONFIG})
    test -d ${auth_dir} || echo "Creating auth dir: ${auth_dir}"
    mkdir -p  ${auth_dir}
    ln -svf ${KUBECONFIG} ${CLUSTER_KUBECONFIG}
    exit 0
}

# installer steps (avoid duplicate on installer options)
# setup   : only setup
# install : only install
# create  : setup + install

installer_step_setup() {
    if [[ ${OPT_KUBECONFIG:-} == true ]]; then
        create_kubeconfig
        return
    fi
    check_cluster_exists_to_create

    render_runner_config

    check_dependencies_to_create
    create_install_config
    create_manifests
    create_patchs_when_create
    if [[ ${OPT_CLUSTER_MANIFESTS_ONLY:-} == true ]]; then
        return
    fi
    if [[ ${OPT_CLUSTER_IGNITION_ONLY:-} == true ]]; then
        create_ignition_files
        return
    fi
}

installer_step_install() {
  create_cluster
}

installer_step_create() {
  installer_step_setup
  installer_step_install
}

# [cluster] sub command handler
installer_cluster_main() {

  case ${OPT_CLUSTER_CMD} in
      "install")
          installer_step_install
      ;;
      "setup")
          installer_step_setup
      ;;
      # ocp installer: create a cluster
      "create")
          installer_step_create
      ;;
      # ocp installer: destroy a cluster
      "destroy"|"delete")
          backup_cluster_installdir
          check_cluster_exists_to_destroy
          destroy_cluster
      ;;
      # send a ping (get nodes/version/etc) to validate if created cluster is live
      "ping")
          echo "TODO: run oc get nodes | whoami ..."
      ;;
  esac
}
