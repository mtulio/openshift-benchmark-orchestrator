#!/bin/sh

#
# Cluster patchs
#

#
# Manifests Patchs / step
#

# Patch Machine Manifest for AWS to add two block devices
patch_machine_aws_2xDevBlocks() {

  local tmpdir=$(mktemp)
  rm -rf ${tmpdir}
  mkdir -p ${tmpdir}

  for machine_manifest in $(ls ${CLUSTER_HOME}/openshift/99_openshift-cluster-api_master-machines-*.yaml); do 
    
    local machine_file=$(basename ${machine_manifest})
    
    # extract header (pre blockDevices)
    sed '1,/blockDevices/!d' ${machine_manifest} > ${tmpdir}/${machine_file}_00

    # duplicate device list block
    cat << EOF > ${tmpdir}/${machine_file}_01-devs
      - ebs:
          encrypted: true
          iops: 0
          kmsKey:
            arn: ""
          volumeType: $(yq -r .controlPlane_vol_type ${CLUSTER_CFG_RENDERED} )
          volumeSize: $(yq -r .controlPlane_vol_size ${CLUSTER_CFG_RENDERED} )
      - deviceName: /dev/xvdb
        ebs:
          encrypted: true
          iops: 0
          kmsKey:
            arn: ""
          volumeType: $(yq -r .controlPlane_vol_type ${CLUSTER_CFG_RENDERED} )
          volumeSize: $(yq -r .controlPlane_vol_size ${CLUSTER_CFG_RENDERED} )

EOF
    # extract footer (post blockDevices
    grep credentialsSecret -A 999 ${machine_manifest} > ${tmpdir}/${machine_file}_03

    # join files
    cat ${tmpdir}/${machine_file}_* > ${tmpdir}/${machine_file}

    echo "Aggregated file created on ${tmpdir}/${machine_file} , overwriting to ${machine_manifest}"
    cp ${tmpdir}/${machine_file} ${machine_manifest}
  done
  rm -rf ${tmpdir}
}

# patch MachineConfig adding extra disk to mount etcd
patch_machineConfig_aws_mount_etcd() {

  # https://docs.fedoraproject.org/en-US/fedora-coreos/storage/
  # https://coreos.github.io/butane/examples/#mirrored-boot-disk
  
  cat << EOF > ${CLUSTER_HOME}/openshift/99_openshift-machineconfig_00-master-etcd.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: master
  name: 00-master-etcd
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      disks:
      - device: /dev/nvme1n1
        wipe_table: true
        partitions:
        - size_mib: 0
          label: etcd
      filesystems:
        - path: /var/lib/etcd
          device: /dev/disk/by-partlabel/etcd
          format: xfs
          wipe_filesystem: true
    systemd:
      units:
        - name: var-lib-etcd.mount
          enabled: true
          contents: |
            [Unit]
            Before=local-fs.target
            [Mount]
            Where=/var/lib/etcd
            What=/dev/disk/by-partlabel/etcd
            [Install]
            WantedBy=local-fs.target
EOF
}

# Run CCO as manual-STS
# https://docs.openshift.com/container-platform/4.9/authentication/managing_cloud_provider_credentials/cco-mode-sts.html
patch_cco_aws_sts() {
  pushd ${CLUSTER_HOME}

  local bin_oc=$(yq -r .oc_path ${CLUSTER_CFG_RENDERED})
  local bin_installer=$(yq -r .installer_path ${CLUSTER_CFG_RENDERED})
  local bin_ccoctl=$(yq -r .ccoctl_path ${CLUSTER_CFG_RENDERED})

  # install
  if [[ ! -x ${bin_ccoctl} ]]; then
    echo "#> Getting CCO IMAGE"
    CCO_IMAGE=$(${bin_oc} adm release info \
                -a ${PULL_SECRET_PATH} \
                --image-for='cloud-credential-operator' \
                ${RELEASE_IMAGE})
    echo "#> Extracting ccoctl binary"
    ${bin_oc} image extract ${CCO_IMAGE} \
      -a ${PULL_SECRET_PATH} \
      --file="/usr/bin/ccoctl"
    mv ./ccoctl ${bin_ccoctl}
    chmod 775 ${bin_ccoctl}
  fi

  #mkdir -p ${CLUSTER_HOME}/_cco-credRequests
  echo "#> Extracting Credential Request manifests from release"
  ${bin_oc} adm release extract \
    -a ${PULL_SECRET_PATH} \
    --credentials-requests \
    --cloud aws \
    --to ${CLUSTER_HOME}/_cco-credRequests \
    ${RELEASE_IMAGE}

  local cluster_name=$(awk '/infrastructureName:/{print $2}' ${CLUSTER_HOME}/manifests/cluster-infrastructure-02-config.yml)
  local cluster_region=$(awk '/region:/{print $2}' ${CLUSTER_HOME}/manifests/cluster-infrastructure-02-config.yml)

  echo "#> Create AWS assets for manual STS: cluster=[${cluster_name}] region=[${cluster_region}]"
  ${bin_ccoctl} aws create-all \
    --name=${cluster_name} \
    --region=${cluster_region} \
    --credentials-requests-dir=${CLUSTER_HOME}/_cco-credRequests \
    --output-dir=${CLUSTER_HOME}/_cco-manifests

  echo "#> Copying manifests to install dir."
  cp -v ${CLUSTER_HOME}/_cco-manifests/manifests/* ${CLUSTER_HOME}/manifests/
  cp -av ${CLUSTER_HOME}/_cco-manifests/tls ${CLUSTER_HOME}/

  popd
}

# Patch CCO S3 bucket to use restrictive policy
patch_cco_aws_sts_s3_restrict() {

  local bucket_name=$(awk '/serviceAccountIssuer:/{print$2}' ${CLUSTER_HOME}/_cco-manifests/manifests/cluster-authentication-02-config.yaml |awk -F'//' '{print$2}' |awk -F'.' '{print$1}')
  local policy_file="${CLUSTER_HOME}/_cco-bucket-policy.json"

  echo "#cco path> Modifying bucket ${bucket_name} to more restrictive policies"
  cat << EOF > ${policy_file}
{
    "Version": "2012-10-17",
    "Id": "SourceIP",
    "Statement": [
        {
            "Sid": "SourceIP",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:Get*",
            "Resource": [
                "arn:aws:s3:::${bucket_name}",
                "arn:aws:s3:::${bucket_name}/*"
            ],
            "Condition": {
                "IpAddress": {
                    "aws:SourceIp": "10.0.0.0/8"
                }
            }
        }
    ]
}
EOF

  echo "#cco patch>> Creating bucket policy: "
  cat ${policy_file}
  aws s3api put-bucket-policy \
    --bucket ${bucket_name} \
    --policy file://${policy_file}

  BLOCK_CFG="BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
  echo "#cco patch>> Removing public access config: ${BLOCK_CFG}"
  aws s3api put-public-access-block \
    --bucket ${bucket_name} \
    --public-access-block-configuration ${BLOCK_CFG}
}

# patch_s3_nobucket() {

#   local bucket_name=$(awk '/serviceAccountIssuer:/{print$2}' ${CLUSTER_HOME}/_cco-manifests/manifests/cluster-authentication-02-config.yaml |awk -F'//' '{print$2}' |awk -F'.' '{print$1}')

#   echo "#> Deleting bucket ${bucket_name} objects"
#   aws s3 rm s3://${bucket_name} --recursive

#   echo "#> Deleting bucket ${bucket_name}"
#   aws s3api delete-bucket \
#     --bucket ${bucket_name}
# }

cluster_hook_preserve_bootstrap() {
  export OPENSHIFT_INSTALL_PRESERVE_BOOTSTRAP=true
}

# WIP
# (lab) Manifest Hook to create OIDC with CloudFront
patch_cco_aws_sts_s3_restrict_cloudfront() {
  local bucket_name=$(awk '/serviceAccountIssuer:/{print$2}' ${CLUSTER_HOME}/_cco-manifests/manifests/cluster-authentication-02-config.yaml |awk -F'//' '{print$2}' |awk -F'.' '{print$1}')
  local policy_file="${CLUSTER_HOME}/_cco-bucket-policy.json"

  # Create CF distribution
  # https://docs.aws.amazon.com/cli/latest/reference/cloudfront/create-cloud-front-origin-access-identity.html
  CFN_ID=$(aws cloudfront create-cloud-front-origin-access-identity \
    --cloud-front-origin-access-identity-config \
      CallerReference="cfn-identity-oidc-${cluster_name}",Comment="CloudFront identity for cluster ${cluster_name}" \
      |jq .CloudFrontOriginAccessIdentity.Id)

  aws cloudfront create-distribution --generate-cli-skeleton

  echo "#cco path> CDN config to host OIDC ${bucket_name} "
  cat << EOF > ${policy_file}
TBD
EOF

  echo "#cco patch>> Creating bucket policy: "
  cat ${policy_file}
  aws s3api put-bucket-policy \
    --bucket ${bucket_name} \
    --policy file://${policy_file}

  BLOCK_CFG="BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true"
  echo "#cco patch>> Removing public access config: ${BLOCK_CFG}"
  aws s3api put-public-access-block \
    --bucket ${bucket_name} \
    --public-access-block-configuration ${BLOCK_CFG}
}
