{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report FIO results for EBS Benchmark on gp2 and gp3\n",
    "\n",
    "Steps to run this report:\n",
    "\n",
    "Option A - run the data to playbook:\n",
    "\n",
    "- Download this playbook\n",
    "- Download the data from [Google Drive](https://drive.google.com/drive/folders/1ADvccAAjdluoB0cJwENCNAZ_Evc7YN6f?usp=sharing)\n",
    "- Make sure that the extracted data will be available on /results (from jupyter container). Eg:\n",
    "```bash\n",
    "tar xf results-b3_loop1.tar.xz -C ./results\n",
    "podman run -v ${PWD}/results:/results:Z <jupyter_container_args>\n",
    "```\n",
    "\n",
    "Option B - load exported playbook with data for each runtime, available on:\n",
    "\n",
    "```bash\n",
    "$ JOB_GROUP=b3_loop1\n",
    "$ results-${JOB_GROUP}.tar.xz/byGroup-${JOB_GROUP}/parser/${JOB_GROUP}.ipynb\n",
    "\n",
    "$ ls -sh byGroup-*/parser/*.ipynb\n",
    "4.2M byGroup-b3_loop10/parser/b3_loop10.ipynb  2.7M byGroup-b3_loop1/parser/b3_loop1.ipynb  3.8M byGroup-b4_loop1/parser/b4_loop1.ipynb  3.9M byGroup-b4_loop5/parser/b4_loop5.ipynb\n",
    "```\n",
    "\n",
    "### Description\n",
    "\n",
    "The detailed description is available on the document [SPLAT-253 - AWS gp3 study case for IPI](https://docs.google.com/document/d/1r_WjugwBZyp508DAv_3PHWlYKhk5TG8DtSq8ZBc80mo/edit#heading=h.8csikri78lve)\n",
    "\n",
    "Scenario (clusters):\n",
    "- c1: OCP cluster with 1x gp2\n",
    "- c2: OCP cluster with 2x gp2 (etcd isolated)\n",
    "- c3: OCP cluster with 1x gp3\n",
    "- c4: OCP cluster with 2x gp3 (etcd isolated)\n",
    "\n",
    "This report aggregates the data collected on FIO tests, that tested all control plane disks on layouts described above.\n",
    "\n",
    "The script to create the \"battery 2\" and collect the data is defined (by WIP script) [here](https://github.com/mtulio/openshift-cluster-benchmark-lab/blob/init/run-test.sh#L250-L271)\n",
    "\n",
    "References:\n",
    " - [FIO doc](https://fio.readthedocs.io/en/latest/fio_doc.html)\n",
    " - This report (notebook): reports/fio-ebs_gp3-b2.ipynb\n",
    " - This report (markdown/exported): docs/examples/fio-ebs_gp3-b2.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "! pip install pandas matplotlib natsort pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tarfile\n",
    "from pprint import pprint\n",
    "\n",
    "from natsort import natsorted\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name_map = {\n",
    "    \"fio_ebs_initialize\": \"0_fio_ebs_initialize\",\n",
    "    \"fio_psync_randwrite\": \"1_fio_psync_randwrite\",\n",
    "    \"fio_libaio_read\": \"2_fio_libaio_read\",\n",
    "    \"fio_libaio_write\": \"3_fio_libaio_write\",\n",
    "    \"fio_libaio_rw\": \"X_fio_libaio_rw\",\n",
    "    \"fio_libaio_randread\": \"4_fio_libaio_randread\",\n",
    "    \"fio_libaio_randwrite\": \"5_fio_libaio_randwrite\",\n",
    "    \"fio_libaio_randrw\": \"6_fio_libaio_randrw\",\n",
    "    \"fio_sync_read\": \"2_fio_sync_read\",\n",
    "    \"fio_sync_write\": \"1_fio_sync_write\",\n",
    "    \"fio_sync_write_alias\": \"4_fio_sync_write_alias\",\n",
    "    \"fio_sync_rw\": \"3_fio_sync_rw\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "#os.environ['JOB_GROUP'] = \"b3_loop10\"\n",
    "#os.environ['JOB_GROUP'] = \"b4_loop5\"\n",
    "job_group=(f\"{os.getenv('JOB_GROUP', 'b3_loop1')}\")\n",
    "\n",
    "# specific for the test env, b3 will test only RW\n",
    "if job_group.startswith(\"b3\"):\n",
    "    job_group_operations=\"rw\"\n",
    "else:\n",
    "    job_group_operations=\"all\" # read,write,sync\n",
    "\n",
    "results_path=(f\"/results/byGroup-{job_group}\")\n",
    "parser_path = (f\"{results_path}/parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_output_path = (f\"{parser_path}/{job_group}.html\")\n",
    "now = datetime.now()\n",
    "html_output = (\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table, td, th {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "\n",
    "table {\n",
    "  width: 100%;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\"\"\")\n",
    "html_output += (\"<h2> FIO Benchmark Report </h2>\")\n",
    "html_output += (\"Generated at: \" + now.strftime(\"%Y-%d-%m, %H:%M:%S\") + \"UTC <br>\")\n",
    "html_output += (f\"\"\"\n",
    "<br>- Job Group (Report Name): {job_group}</>\n",
    "<br>- Job Group Operations   : {job_group_operations}</>\n",
    "<br>- Results base path      : {results_path}</>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [' ', '(', ')']\n",
    "def output_add_table(html_output, title=\"\", desc=\"\", data=\"\"):\n",
    "    html_output += (f\"<br><h4>{title}</h4>\")\n",
    "    #link = title\n",
    "    #for c in chars:\n",
    "    #    link = link.replace(c, '-')\n",
    "    #html_output += (f\"\"\"<p><a href='#{link}'><h4>{title}</h4></a></p>\"\"\")\n",
    "    html_output += (f\"{desc}\")\n",
    "    html_output += (f\"<br>{data}\")\n",
    "    return html_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_result_files(base_path, results=[], start_str=\"\", contains_str=\"\", extension=\"\", ignore_str=None):\n",
    "    \"\"\"\n",
    "    Generic lookup based on filters criteria\n",
    "    \"\"\"\n",
    "    for res in os.listdir(base_path):\n",
    "        # check prefix\n",
    "        if not res.startswith(filter_results_by_battery):\n",
    "            #print(f\"01: {res}\")\n",
    "            continue\n",
    "\n",
    "        # check extension\n",
    "        if not res.endswith(extension):\n",
    "            #print(f\"02: {res}\")\n",
    "            continue\n",
    "\n",
    "        # check filter\n",
    "        if contains_str not in res:\n",
    "            #print(f\"03: {res}\")\n",
    "            continue\n",
    "\n",
    "        # ignore strings\n",
    "        if (ignore_str != None) and (ignore_str in res):\n",
    "            #print(f\"04: {res}\")\n",
    "            continue\n",
    "\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom node alias builder. To get shorter columns =)\n",
    "ocp_default_subnets = [\n",
    "    {\n",
    "        \"azName\": \"us-east-1a-public\",\n",
    "        \"azId\": \"use1-az4\",\n",
    "        \"cidr\": \"10.0.0.0/20\",\n",
    "        \"cidr_3o_start\": 0,\n",
    "        \"cidr_3o_end\": 127\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1b-public\",\n",
    "        \"azId\": \"use1-az6\",\n",
    "        \"cidr\": \"10.0.16.0/20\",\n",
    "        \"cidr_3o_start\": 16,\n",
    "        \"cidr_3o_end\": 31\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1c-public\",\n",
    "        \"azId\": \"use1-az1\",\n",
    "        \"cidr\": \"10.0.32.0/20\",\n",
    "        \"cidr_3o_start\": 32,\n",
    "        \"cidr_3o_end\": 47\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1a-private\",\n",
    "        \"azId\": \"use1-az4\",\n",
    "        \"cidr\": \"10.0.128.0/20\",\n",
    "        \"cidr_3o_start\": 128,\n",
    "        \"cidr_3o_end\": 143\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1b-private\",\n",
    "        \"azId\": \"use1-az6\",\n",
    "        \"cidr\": \"10.0.144.0/20\",\n",
    "        \"cidr_3o_start\": 144,\n",
    "        \"cidr_3o_end\": 159\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1c-private\",\n",
    "        \"azId\": \"use1-az2\",\n",
    "        \"cidr\": \"10.0.160.0/20\",\n",
    "        \"cidr_3o_start\": 160,\n",
    "        \"cidr_3o_end\": 175\n",
    "    }\n",
    "]\n",
    "\n",
    "def locate_azId_by_hostname(hostname):\n",
    "    \"\"\"\n",
    "    Assume AzId by hostname. OCP, by default, will deploy first on AzName=a and so on, with standard cidr,\n",
    "    so discovery it in us-east-1 in a standard IPI is easy;\n",
    "    \"\"\"\n",
    "    hostname_ip = (hostname.split('ip-')[1].split('.ec2.internal')[0])\n",
    "    hostname_ip3o = int((hostname_ip.split('-')[2]))\n",
    "    hostname_netIp = (f\"{int((hostname_ip.split('-')[2]))}-{int((hostname_ip.split('-')[3]))}\")\n",
    "    for net in ocp_default_subnets:\n",
    "        if (hostname_ip3o >= net['cidr_3o_start']) and (hostname_ip3o <= net['cidr_3o_end']):\n",
    "            return (net['azId'], net['azName'], hostname_netIp)\n",
    "\n",
    "    return ('AzNotFound', 'NA', hostname_netIp)\n",
    "\n",
    "\n",
    "def find_node_alias_by_hostname(hostname=\"\", add_prefix=\"\", add_suffix=\"\", fmt=\"azId\"):\n",
    "    # assuming all AWS node hostname starts with 'ip-...'\n",
    "    if hostname.startswith('ip-'):\n",
    "        azId, azName, netIp = locate_azId_by_hostname(hostname)\n",
    "        if fmt == \"azId_ipNet\": #> '{region_id}-{az_id}_{ip3o}-{ip4o}'\n",
    "            return (f\"{add_prefix}{azId}_{netIp}{add_suffix}\")\n",
    "        elif fmt == \"azIdShort_ipNet\":  #> '{az_id}_{ip3o}-{ip4o}'\n",
    "            return (f\"{add_prefix}{azId.split('-')[1]}_{netIp}{add_suffix}\")\n",
    "        elif fmt == \"ipNet_azIdShort\":  #> '{ip3o}-{ip4o}_{az_id}'\n",
    "            return (f\"{add_prefix}{netIp}_{azId.split('-')[1]}{add_suffix}\")\n",
    "        else: # \"azId\" #> '{region_id}-{az_id}'\n",
    "            return (f\"{add_prefix}{azId}{add_suffix}\")\n",
    "    \n",
    "    # default: not transformations\n",
    "    return hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, name, cluster):\n",
    "        self.node_name=name\n",
    "        self.node_alias=\"\"\n",
    "        self.cluster=cluster\n",
    "        self.cluster_full=\"\"\n",
    "\n",
    "        self.metrics=[]\n",
    "\n",
    "    def add_metric(self, **kwargs):\n",
    "        #print(f\"Adding metric [{kwargs['metric']}]\")\n",
    "        self.metrics.append({\n",
    "            \"job_name\": kwargs[\"job_name\"],\n",
    "            \"job_group\": kwargs[\"job_group\"],\n",
    "            \"task_name\": task_name_map[kwargs[\"task_name\"]],\n",
    "            \"task_group\": kwargs[\"task_group\"],\n",
    "            \"task_execId\": kwargs[\"execId\"],\n",
    "            \"timestamp\": kwargs[\"timestamp\"],\n",
    "            \"metric\": kwargs[\"metric\"],\n",
    "            \"value\": kwargs[\"value\"],\n",
    "        })\n",
    "\n",
    "\n",
    "class Nodes(object):\n",
    "    def __init__(self):\n",
    "        self.nodes={}\n",
    "    \n",
    "    def add_node(self, node, cluster):\n",
    "        try:\n",
    "            node = self.nodes[node]\n",
    "        except KeyError:\n",
    "            self.nodes[node] = Node(node, cluster)\n",
    "            print(f\"Node [{node}] added\")\n",
    "            #self.nodes[node].node_alias = find_node_alias_by_hostname(hostname=node, add_prefix=f\"{cluster}_\", fmt=\"azIdShort_ipNet\")\n",
    "            self.nodes[node].node_alias = find_node_alias_by_hostname(hostname=node, add_prefix=f\"{cluster}_\")\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "    def get_node(self, node):\n",
    "        try:\n",
    "            return self.nodes[node]\n",
    "        except:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_results_fio_runtime(node, data_path, job_info):\n",
    "    \"\"\"\n",
    "    FIO runtime log parser. See below some examples of data.\n",
    "    sample of header line:\n",
    "    #cluster=c1gp2x1> Running task [fio_psync_randwrite] on node [ip-10-0-142-138.ec2.internal], registering on log file ./.local/results/byGroup-b3_loop1/fio_stdout-c1-ip-10-0-142-138.ec2.internal.txt\n",
    "    \n",
    "    sample of metric line:\n",
    "    [0] <=> ip-10-0-142-138 <=> Thu Sep  9 13:51:16 UTC 2021 <=>  13:51:16 up 32 min,  0 users,  load average: 1.27, 0.83, 1.21 \n",
    "    [1] <=> ip-10-0-142-138 <=> Thu Sep  9 14:03:16 UTC 2021 <=>  14:03:16 up 44 min,  0 users,  load average: 0.87, 2.34, 2.93 \n",
    "    \"\"\"\n",
    "\n",
    "    job_name, job_group = job_info\n",
    "    task_group = \"fio_runtime\"\n",
    "    with open(data_path) as f:\n",
    "        last_job = ''\n",
    "        time_init = None\n",
    "        current_task = \"\"\n",
    "        for line in f.readlines():\n",
    "            # parse line : [...] Running task [fio_psync_randwrite] [...],\n",
    "            if 'Running task [' in line:\n",
    "                current_task = line.split('Running task [')[1].split(']')[0]\n",
    "                if node.cluster_full == \"\":\n",
    "                    node.cluster_full = line.split('#cluster=')[1].split('>')[0]\n",
    "                continue\n",
    "            if line.startswith('['):\n",
    "                # extract jobId, time and Load1\n",
    "                jobId = line.split(' <=> ')[0].replace('[','').replace(']','')\n",
    "                load1 = line.split(' <=> ')[3].split('load average: ')[1].split(',')[0]\n",
    "                ts = line.split(' <=> ')[2]\n",
    "                node.add_metric(job_name=job_name,\n",
    "                                job_group=job_group,\n",
    "                                task_name=current_task,\n",
    "                                task_group=task_group,\n",
    "                                execId=jobId,\n",
    "                                timestamp=ts,\n",
    "                                metric='load1',\n",
    "                                value=load1)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIO Payload (Sample)\n",
    "\n",
    "Payload sample to build the metric parser fn()\n",
    "\n",
    "> Open sample FIO result `reports/sample-fio_psync_randwrite.json` from task `fio_psync_randwrite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_results_fio_tasks(node, data_path, job_info):\n",
    "    \"\"\"\n",
    "    FIO payload parser.\n",
    "    Walk through fio result dir and load JSON files with FIO results,\n",
    "    returning only desired metrics for each test.\n",
    "    \"\"\"\n",
    "    job_name, job_group, task_name = job_info\n",
    "    task_group = \"fio_tasks\"\n",
    "\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                fpath=os.path.join(root, file)\n",
    "                with open(fpath, 'r') as f:\n",
    "                    res_payload=json.loads(f.read())\n",
    "\n",
    "                    # Extract jobId from different standards (latest is fio_io_)\n",
    "                    try:\n",
    "                        jobId = res_payload['jobs'][0]['jobname'].split('fio_io_')[1]\n",
    "                    except Exception as e:\n",
    "                        raise e\n",
    "                        \n",
    "                    #print(job_group, job_name, task_name, task_group, jobId)\n",
    "                    #pprint(res_payload)\n",
    "\n",
    "                    ts = res_payload['timestamp']\n",
    "                    metrics_collection = {\n",
    "                        \"global\": {\n",
    "                            \"read_ios\": res_payload['disk_util'][0]['read_ios'],\n",
    "                            \"write_ios\": res_payload['disk_util'][0]['read_ios'],\n",
    "                            \"bs\": res_payload['global options']['bs'],\n",
    "                            \"ioengine\": res_payload['global options']['ioengine'],\n",
    "                            \"numjobs\": res_payload['global options']['numjobs'],\n",
    "                            \"runtime\": res_payload['global options']['runtime'],\n",
    "                            \"rw\": res_payload['global options']['rw'],\n",
    "                            \"size\": res_payload['global options']['size'],\n",
    "                            \"jobname\": res_payload['jobs'][0]['jobname'],\n",
    "                        },\n",
    "                        \"values\": {\n",
    "                            \"elapsed\": res_payload['jobs'][0]['elapsed'],\n",
    "                            \"latency_ms\": res_payload['jobs'][0]['latency_ms'],\n",
    "                            \"read_bw\": res_payload['jobs'][0]['read']['bw'],\n",
    "                            \"read_iops\": res_payload['jobs'][0]['read']['iops'],\n",
    "                            \"read_total_ios\": res_payload['jobs'][0]['read']['total_ios'],\n",
    "                            \"read_lat_ms_min\": (float(res_payload['jobs'][0]['read']['lat_ns']['min'])/1e+6),\n",
    "                            \"read_lat_ms_max\": (float(res_payload['jobs'][0]['read']['lat_ns']['max'])/1e+6),\n",
    "                            \"read_lat_ms_mean\": (float(res_payload['jobs'][0]['read']['lat_ns']['mean'])/1e+6),\n",
    "                            \"read_clat_ms_p99\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"read_clat_ms_p99.9\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"read_clat_ms_p99.99\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"read_clat_ms_stddev\": (float(res_payload['jobs'][0]['read']['clat_ns']['stddev'])/1e+6),\n",
    "                            \"write_bw\": res_payload['jobs'][0]['write']['bw'],\n",
    "                            \"write_iops\": res_payload['jobs'][0]['write']['iops'],\n",
    "                            \"write_total_ios\": res_payload['jobs'][0]['write']['total_ios'],\n",
    "                            \"write_lat_ms_min\": (float(res_payload['jobs'][0]['write']['lat_ns']['min'])/1e+6),\n",
    "                            \"write_lat_ms_max\": (float(res_payload['jobs'][0]['write']['lat_ns']['max'])/1e+6),\n",
    "                            \"write_lat_ms_mean\": (float(res_payload['jobs'][0]['write']['lat_ns']['mean'])/1e+6),\n",
    "                            \"write_clat_ms_p99\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"write_clat_ms_p99.9\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"write_clat_ms_p99.99\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"write_clat_ms_stddev\": (float(res_payload['jobs'][0]['write']['clat_ns']['stddev'])/1e+6),\n",
    "                            \"sync_total_ios\": res_payload['jobs'][0]['sync']['total_ios'],\n",
    "                            \"sync_lat_ms_min\": (float(res_payload['jobs'][0]['sync']['lat_ns']['min'])/1e+6),\n",
    "                            \"sync_lat_ms_max\": (float(res_payload['jobs'][0]['sync']['lat_ns']['max'])/1e+6),\n",
    "                            \"sync_lat_ms_mean\": (float(res_payload['jobs'][0]['sync']['lat_ns']['mean'])/1e+6),\n",
    "                            \"sync_lat_ms_p99\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"sync_lat_ms_p99.9\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"sync_lat_ms_p99.99\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"sync_lat_ms_stddev\": (float(res_payload['jobs'][0]['sync']['lat_ns']['stddev'])/1e+6),\n",
    "                            \"cpu_sys\": res_payload['jobs'][0]['sys_cpu'],\n",
    "                            \"cpu_usr\": res_payload['jobs'][0]['usr_cpu'],\n",
    "                            \"cpu_ctx\": res_payload['jobs'][0]['ctx']\n",
    "                        }\n",
    "                    }\n",
    "                    node.add_metric(job_name=job_name,\n",
    "                                job_group=job_group,\n",
    "                                task_name=task_name,\n",
    "                                task_group=task_group,\n",
    "                                execId=jobId,\n",
    "                                timestamp=ts,\n",
    "                                metric='collection',\n",
    "                                value=metrics_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metric_collection(data, metric_name, is_collection=True):\n",
    "    \"\"\"\n",
    "    Filter desired {metric_name}, extract the jobs (rows) for each cluster (columns),\n",
    "    and return the data frame.\n",
    "    JobId | {cluster1}  | [...clusterN |]\n",
    "    #id   | metricValue | [...metricValue |]\n",
    "    \"\"\"\n",
    "    data_metric = {}\n",
    "    \n",
    "    def insert_metric(nid, jid, val):\n",
    "        try:\n",
    "            job = data_metric[jid]\n",
    "        except KeyError:\n",
    "            data_metric[jid] = {\n",
    "                \"job_Id\": jid\n",
    "            }\n",
    "            job = data_metric[jid]\n",
    "            pass\n",
    "        job[nid] = val\n",
    "\n",
    "    for n in data.nodes.keys():\n",
    "        node = data.nodes[n]\n",
    "        for metric in node.metrics:\n",
    "\n",
    "            job_id = (f\"{metric['task_name']}#{metric['task_execId']}\")\n",
    "            \n",
    "            # get simple metric value (metric != 'collection')\n",
    "            if not(is_collection) or (metric['metric'] != \"collection\"):\n",
    "                if metric['metric'] == metric_name:\n",
    "                    insert_metric(node.node_alias, job_id, metric['value'])\n",
    "                continue\n",
    "\n",
    "            #print(metric['metric'])\n",
    "            #print(node.node_alias, metric_name)\n",
    "            #print(metric['value']['values'])\n",
    "            insert_metric(node.node_alias, job_id, metric['value']['values'][metric_name])\n",
    "            #jid[node.node_alias] = metric['value']['values'][metric_name]\n",
    "\n",
    "    data_pd = []\n",
    "    for dk in natsorted(data_metric.keys()):\n",
    "        data_pd.append(data_metric[dk])\n",
    "\n",
    "    # create data frame and force job_id as first column\n",
    "    df = pd.read_json(json.dumps(data_pd))\n",
    "    #columns = \n",
    "    #print(columns)\n",
    "    return df.reindex(['job_Id'] + natsorted(df.columns.drop('job_Id')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_style_high(val, value_yellow=None, value_red=None, value_greenS=None, value_greenH=None, invert=False):\n",
    "    \"Data frame styling / cell formating\"\n",
    "    color_map = {\n",
    "        \"green_soft\": \"#DAF7A6\",\n",
    "        \"green_hard\": \"#02FC11\",\n",
    "        \"red_hard\": \"#FC5A5A\",\n",
    "        \"yellow_hard\": \"#E6ED02\",\n",
    "    }\n",
    "    color = None\n",
    "\n",
    "    # ignore 0 values\n",
    "    if (invert) and (val == 0.0):\n",
    "        return color\n",
    "    \n",
    "    # yellow (high)\n",
    "    if ((value_yellow != None) and not(invert)) and (val >=  value_yellow):\n",
    "        color = color_map[\"yellow_hard\"]\n",
    "    if ((value_yellow != None) and (invert)) and (val <=  value_yellow):\n",
    "        color = color_map[\"yellow_hard\"]\n",
    "    \n",
    "    # red (very high)\n",
    "    if ((value_red != None) and not(invert))  and (val >=  value_red):\n",
    "        color = color_map[\"red_hard\"]\n",
    "    if ((value_red != None) and (invert)) and (val <=  value_red):\n",
    "        color = color_map[\"red_hard\"]\n",
    "\n",
    "    # blue (low)\n",
    "    if ((value_greenS != None) and not(invert))  and (val <=  value_greenS):\n",
    "        color = color_map[\"green_soft\"]\n",
    "    if ((value_greenS != None) and (invert)) and (val >=  value_greenS):\n",
    "        color = color_map[\"green_soft\"]\n",
    "\n",
    "    # green (very low)\n",
    "    if ((value_greenH != None) and not(invert))  and (val <=  value_greenH):\n",
    "        color = color_map[\"green_hard\"]\n",
    "    if ((value_greenH != None) and (invert)) and (val >=  value_greenH):\n",
    "        color = color_map[\"green_hard\"]\n",
    "        \n",
    "    # default color\n",
    "    if color == None:\n",
    "        return color\n",
    "   \n",
    "    #return f\"color: {color}\"\n",
    "    return f\"background-color: {color}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery and Load results for 'fio' tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "#battery_id = \"b2\"\n",
    "filter_results_by_battery=\"\"\n",
    "\n",
    "nodes = {}\n",
    "\n",
    "# Runtime runtime, custom stdout collecting when FIO jobs was running\n",
    "fio_runtime = {}\n",
    "\n",
    "# FIO Runtime log parser\n",
    "result_fio_runtime_files = []\n",
    "\n",
    "# Nodes entity\n",
    "nodes = Nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fio_runtime_files = lookup_result_files(results_path,\n",
    "                                                results=result_fio_runtime_files,\n",
    "                                                start_str=filter_results_by_battery,\n",
    "                                                contains_str=\"fio_stdout\",\n",
    "                                                extension=\".txt\"\n",
    "                                               )\n",
    "#len(result_fio_runtime_files)\n",
    "html_output += (f\"\"\"\n",
    "<br>- Total of FIO runtime logs processed: {len(result_fio_runtime_files)}</>\n",
    "<br>- FIO runtime logs processed: {result_fio_runtime_files}</>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fio_runtime_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build metrics from FIO Runtime (stdout parser)\n",
    "for res in result_fio_runtime_files:\n",
    "    task_name = f\"{res.split('-')[0]}\"\n",
    "    job_name = f\"{res.split('-')[1]}\"\n",
    "    node_name = f\"{res.split(job_name+'-')[1].split('.txt')[0]}\"\n",
    "\n",
    "    nodes.add_node(node_name, job_name)\n",
    "\n",
    "    parser_results_fio_runtime(nodes.get_node(node_name), f\"{results_path}/{res}\", job_info=(job_name, job_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes['ip-10-0-173-135.ec2.internal'].node_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes['ip-10-0-166-6.ec2.internal'].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIO raw payload: files is saved on the format: {battery_id}_{cluster_id}-fio-{hostname}.tar.gz ;\n",
    "# TODO unpack it, currently it should be done manually\n",
    "results_dirs_fio = []\n",
    "results_dirs_fio = lookup_result_files(results_path,\n",
    "                                        results=results_dirs_fio,\n",
    "                                        start_str=\"fio_\",\n",
    "                                        contains_str=\"fio_\",\n",
    "                                        extension=\"tar.gz\",\n",
    "                                        ignore_str=\".txt\"\n",
    "                                       )\n",
    "len(results_dirs_fio)\n",
    "html_output += (f\"\"\"\n",
    "<br>- Total of FIO task results' files processed: {len(results_dirs_fio)}</>\n",
    "<br>- FIO task results' files processed: {results_dirs_fio}</>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIO payload reader:\n",
    "# 1. extract FIO results tarbal (saved by task)\n",
    "# 2. lookup all JSON files for each result task\n",
    "# 3. parse FIO payload: extract only desired metrics to be used in this report\n",
    "for res in results_dirs_fio:\n",
    "\n",
    "    task_name = f\"{res.split('-')[0]}\"\n",
    "    job_name = f\"{res.split('-')[1]}\"\n",
    "    node_name = f\"{res.split(job_name+'-')[1].split('.tar.gz')[0]}\"\n",
    "    \n",
    "    nodes.add_node(node_name, job_name)\n",
    "\n",
    "    # crate parser result dir and unpack it\n",
    "    dest_path_res = f\"{parser_path}/{res.split('.tar.gz')[0]}\"\n",
    "    \n",
    "    # dependens: mkdir .local/results/byGroup-b3_loop10/parser && chmod o+rw .local/results/byGroup-b3_loop10/parser\n",
    "    !mkdir -p f\"{dest_path_res}\"\n",
    "    \n",
    "    try:\n",
    "        if res.endswith('tar.gz'):\n",
    "            tar = tarfile.open(f\"{results_path}/{res}\")\n",
    "            tar.extractall(path=dest_path_res)\n",
    "            tar.close()\n",
    "    except:\n",
    "        # when the file is not found, or corrupted. Add empty metric\n",
    "        nodes.get_node(node_name).add_metric(\n",
    "            job_name=job_name,\n",
    "            job_group=job_group,\n",
    "            task_name=task_name,\n",
    "            task_group=\"fio_tasks\",\n",
    "            execId='',\n",
    "            timestamp='',\n",
    "            metric='empty',\n",
    "            value=''\n",
    "        )\n",
    "        print(job_group, job_name, task_name, \"fio_tasks\", node_name)\n",
    "        print(f\"ERR, dataset not found or corrupted [{res}]; Empty metric added\")\n",
    "        continue\n",
    "\n",
    "    # parser    \n",
    "    parser_results_fio_tasks(nodes.get_node(node_name), f\"{dest_path_res}\", job_info=(job_name, job_group, task_name))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.nodes[node_name].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_fio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for 'fio'\n",
    "\n",
    "As described, the tests was done in 4 clusters in two disk layouts (single disk, etcd isolated) using gp2 and gp3. The volume has same capacity using standard values for IOPS and throughput (gp3)\n",
    "\n",
    "- Total of FIO consecutive tests: 50\n",
    "- Max IOPS on all jobs job: ~1.5/2k IOPS\n",
    "- Max IOPS for gp2 device: 386 (capacity=128GiB, throughput*=128 MiB/s)\n",
    "- Max IOPS for gp3 device: 3000 (capacity=128GiB, throughput=120MiB/s) \n",
    "\n",
    "\\*[Important note from AWS doc](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html): \n",
    "*\"The throughput limit is between 128 MiB/s and 250 MiB/s, depending on the volume size. Volumes smaller than or equal to 170 GiB deliver a maximum throughput of 128 MiB/s. Volumes larger than 170 GiB but smaller than 334 GiB deliver a maximum throughput of 250 MiB/s if burst credits are available. Volumes larger than or equal to 334 GiB deliver 250 MiB/s regardless of burst credits. gp2 volumes that were created before December 3, 2018 and that have not been modified since creation might not reach full performance unless you modify the volume.\"*\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manifest data\n",
    "import yaml\n",
    "\n",
    "node_data = []\n",
    "for nm in list(nodes.nodes.keys()):\n",
    "    node = nodes.nodes[nm]\n",
    "\n",
    "    # output of : oc get infrastructures -o yaml > {res_path}/infrastructures.yaml\n",
    "    with open(f\"{results_path}/cluster-manifests-{node.cluster_full}/infrastructures.yaml\", \"r\") as stream:\n",
    "        try:\n",
    "            yd = yaml.safe_load(stream)\n",
    "            platform = yd['items'][0]['status']['platform']\n",
    "            infraName = yd['items'][0]['status']['infrastructureName']\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    # output of : oc get machines -n openshift-machine-api -o yaml > {res_path}/machines.yaml\n",
    "    with open(f\"{results_path}/cluster-manifests-{node.cluster_full}/machines.yaml\", \"r\") as stream:\n",
    "        try:\n",
    "            yd = yaml.safe_load(stream)\n",
    "            for m in yd['items']:\n",
    "                if m['status']['nodeRef']['name'] == nm:\n",
    "                    mName = m['metadata']['name']\n",
    "                    mType = m['spec']['providerSpec']['value']['instanceType']\n",
    "                    vType = m['spec']['providerSpec']['value']['blockDevices'][0]['ebs']['volumeType']\n",
    "                    vSize = m['spec']['providerSpec']['value']['blockDevices'][0]['ebs']['volumeSize']\n",
    "                    azName = m['spec']['providerSpec']['value']['placement']['availabilityZone']\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "    # output of : oc get clusterversion -o yaml > {res_path}/clusterversion.yaml\n",
    "    with open(f\"{results_path}/cluster-manifests-{node.cluster_full}/clusterversion.yaml\", \"r\") as stream:\n",
    "        try:\n",
    "            yd = yaml.safe_load(stream)\n",
    "            cversion = yd['items'][0]['status']['desired']['version']\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            \n",
    "    node_data.append({\n",
    "        \"hostname\": node.node_name,\n",
    "        \"machineName\": mName,\n",
    "        \"node_alias\": node.node_alias,\n",
    "        \"AZ\": azName,\n",
    "        \"job\": node.cluster,\n",
    "        \"cluAlias\": node.cluster,\n",
    "        \"cluName\": node.cluster_full,\n",
    "        \"infraName\": infraName,\n",
    "        \"cluVersion\": cversion,\n",
    "        \"platform\": platform,\n",
    "        \"vmType\": mType,\n",
    "        \"volType\": vType,\n",
    "        \"volSzGB\": vSize,\n",
    "                \n",
    "    })\n",
    "df = pd.read_json(json.dumps(node_data))\n",
    "display(df.sort_values(by=['node_alias']))\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"Node Inventory\",\n",
    "                               desc=\"Job inventory of infrastructure that hosted the tests\",\n",
    "                               data=df.sort_values(by=['node_alias']).style.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe custom filters\n",
    "df_filter_az2 = [\"c1_use1-az2\", \"c2_use1-az2\", \"c3_use1-az2\", \"c4_use1-az2\"]\n",
    "df_filter_az4 = [\"c1_use1-az4\", \"c2_use1-az4\", \"c3_use1-az4\", \"c4_use1-az4\"]\n",
    "df_filter_az6 = [\"c1_use1-az6\", \"c2_use1-az6\", \"c3_use1-az6\", \"c4_use1-az6\"]\n",
    "df_filter_gp2 = [\"c1_use1-az2\", \"c1_use1-az4\", \"c1_use1-az6\", \"c2_use1-az2\", \"c2_use1-az4\", \"c2_use1-az6\",]\n",
    "df_filter_gp3 = [\"c3_use1-az2\", \"c3_use1-az4\", \"c3_use1-az6\", \"c4_use1-az2\", \"c4_use1-az4\", \"c4_use1-az6\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_drop_zero_rows(df):\n",
    "    \"\"\"\n",
    "    Replace NaN to 0 from a data frame (df),\n",
    "    and remove **rows** that contains **only** 0.\n",
    "    Return the changed data frame.\n",
    "    \"\"\"\n",
    "    nan_value = float(\"NaN\")\n",
    "    df.replace(nan_value, 0, inplace=True)\n",
    "    deleteIndexes=[]\n",
    "    for col in df.columns.drop('job_Id'):\n",
    "        indexNames = df[(df[col] == 0)].index\n",
    "        if len(indexNames) == 0:\n",
    "            continue\n",
    "        if len(deleteIndexes) == 0:\n",
    "            deleteIndexes = indexNames\n",
    "        deleteIndexes = set(deleteIndexes).intersection(indexNames)\n",
    "    \n",
    "    print(f\"Detected [{len(deleteIndexes)}] rows to be deleted: {deleteIndexes}. Droping it...\\n\")\n",
    "    df.drop(deleteIndexes , inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_oper_for_metric(mp, sync=\"lat\"):\n",
    "    delimiter=\"_\"\n",
    "    if mp == \"\":\n",
    "        delimiter=\"\"\n",
    "    d = {\n",
    "        \"write\": f\"{mp}{delimiter}\",\n",
    "        \"read\": f\"{mp}{delimiter}\"\n",
    "    }\n",
    "    if job_group_operations == \"rw\":\n",
    "        return d\n",
    "\n",
    "    d['sync']=f\"{sync}{delimiter}\"\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (Lattency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job specific\n",
    "metric_prefix=\"lat\"\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix)\n",
    "\n",
    "for op in oper_lat_prefix.keys():\n",
    "    # measurement metric\n",
    "    mm=\"mean\"\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_{mm}\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    display(df.plot(title=f\"{op} {mm} by Node (all)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az2).plot(title=f\"{op} {mm} by Node (az2)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az4).plot(title=f\"{op} {mm} by Node (az4)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az6).plot(title=f\"{op} {mm} by Node (az6)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp2).plot(title=f\"{op} {mm} by Node (gp2)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp3).plot(title=f\"{op} {mm} by Node (gp3)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    \n",
    "    print(f\"\\n>> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'), value_yellow=5.4, value_red=6).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_prefix=\"lat\"\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix)\n",
    "\n",
    "for op in oper_lat_prefix.keys():\n",
    "    mm=\"max\"\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_{mm}\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                            value_yellow=20, value_red=50).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (Percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_prefix=\"clat\"\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix, sync=\"lat\")\n",
    "print(oper_lat_prefix)\n",
    "for op in oper_lat_prefix.keys():\n",
    "    mm=\"p99\"\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_{mm}\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    th_yellow=5\n",
    "    th_red=10.0\n",
    "\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                            value_yellow=th_yellow, value_red=th_red).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    display(df.plot(title=f\"{op} {mm} by Node (all)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az2).plot(title=f\"{op} {mm} by Node (az2)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az4).plot(title=f\"{op} {mm} by Node (az4)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az6).plot(title=f\"{op} {mm} by Node (az6)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp2).plot(title=f\"{op} {mm} by Node (gp2)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp3).plot(title=f\"{op} {mm} by Node (gp3)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    \n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=f\"{dfs.render()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_prefix=\"clat\"\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix, sync=\"lat\")\n",
    "\n",
    "for op in oper_lat_prefix.keys():\n",
    "    mm=\"p99.9\"\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_{mm}\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    display(df.plot(title=f\"{op} {mm} by Node (all)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az2).plot(title=f\"{op} {mm} by Node (az2)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az4).plot(title=f\"{op} {mm} by Node (az4)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az6).plot(title=f\"{op} {mm} by Node (az6)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp2).plot(title=f\"{op} {mm} by Node (gp2)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp3).plot(title=f\"{op} {mm} by Node (gp3)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    \n",
    "    print(f\">> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'), value_yellow=10, value_red=20.0).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_prefix=\"clat\"\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix, sync=\"lat\")\n",
    "\n",
    "for op in oper_lat_prefix.keys():\n",
    "    mm=\"stddev\"\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_{mm}\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                            value_yellow=1, value_red=2).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_prefix=\"\"\n",
    "# there's no sync for IOPS metric\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix, sync=\"\")\n",
    "\n",
    "for op in oper_lat_prefix.keys():\n",
    "    # there's no sync for IOPS metric\n",
    "    if op == \"sync\":\n",
    "        continue\n",
    "    \n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}iops\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    display(df.plot(title=f\"{op} IOPS by Node (all)\", xlabel=\"job_Id\", ylabel=\"Time (ms)\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az2).plot(title=f\"{op} IOPS by Node (az2)\", xlabel=\"job_Id\", ylabel=\"IOPS\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az4).plot(title=f\"{op} IOPS by Node (az4)\", xlabel=\"job_Id\", ylabel=\"IOPS\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_az6).plot(title=f\"{op} IOPS by Node (az6)\", xlabel=\"job_Id\", ylabel=\"IOPS\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp2).plot(title=f\"{op} IOPS by Node (gp2)\", xlabel=\"job_Id\", ylabel=\"IOPS\", fontsize=\"10\", figsize=(25,5)))\n",
    "    display(df.filter(items=df_filter_gp3).plot(title=f\"{op} IOPS by Node (gp3)\", xlabel=\"job_Id\", ylabel=\"IOPS\", fontsize=\"10\", figsize=(25,5)))\n",
    "    \n",
    "    print(f\">> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                            value_yellow=2000, value_red=1000,\n",
    "                            value_greenH=2950, value_greenS=2900,\n",
    "                            invert=True).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_prefix=\"\"\n",
    "# there's no sync for TOTAL IOs metric\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix, sync=\"\")\n",
    "\n",
    "for op in oper_lat_prefix.keys():\n",
    "    # there's no sync for TOTAL IOs metric\n",
    "    if op == \"sync\":\n",
    "        continue\n",
    "\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}total_ios\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                            value_yellow=200000, value_red=100000,\n",
    "                            value_greenS=500000, value_greenH=530000,\n",
    "                            invert=True).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_prefix=\"\"\n",
    "# there's no sync for BW metric\n",
    "oper_lat_prefix = build_oper_for_metric(metric_prefix, sync=\"\")\n",
    "\n",
    "for op in oper_lat_prefix.keys():\n",
    "    # there's no sync for BW metric\n",
    "    if op == \"sync\":\n",
    "        continue\n",
    "\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}bw\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = df_drop_zero_rows(aggregate_metric_collection(nodes, f\"{metric}\"))\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                            value_yellow=20000, value_red=10000, value_greenS=40000,\n",
    "                            value_greenH=47500, invert=True).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_ctx\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "\n",
    "print(f\">> {title}\")\n",
    "dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                        value_yellow=530000, value_red=537500).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_sys\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "#display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.25, value_red=0.50, value_greenS=0.1, value_greenH=0.04))\n",
    "dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.25, value_red=0.50, value_greenS=0.1, value_greenH=0.04).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_usr\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "\n",
    "print(f\">> {title}\")\n",
    "dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                        value_yellow=0.1, value_red=0.2, value_greenS=0.05,\n",
    "                        value_greenH=0.02).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"load1\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\", is_collection=False)\n",
    "\n",
    "print(f\">> {title}\")\n",
    "\n",
    "dfs = df.style.applymap(_df_style_high, subset=df.columns.drop('job_Id'),\n",
    "                        value_yellow=2, value_red=4, value_greenS=1,\n",
    "                        value_greenH=0.5).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to HTML (ps: nbconvert is truncing the tables)\n",
    "html_output += \"\"\"\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(html_output_path, 'w') as f:\n",
    "    f.write(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
