{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report FIO results for EBS Benchmark on gp2 and gp3\n",
    "\n",
    "Steps to run this report:\n",
    "\n",
    "> TODO\n",
    "\n",
    "### Description:\n",
    "\n",
    "Scenario (clusters):\n",
    "- c1: OCP cluster with 1x gp2\n",
    "- c2: OCP cluster with 2x gp2 (etcd isolated)\n",
    "- c3: OCP cluster with 1x gp3\n",
    "- c4: OCP cluster with 2x gp3 (etcd isolated)\n",
    "\n",
    "This report aggregates the data collected on FIO tests, that tested all control plane disks on layouts described above.\n",
    "\n",
    "The script to create the \"battery 2\" and collect the data is defined (by WIP script) [here](https://github.com/mtulio/openshift-cluster-benchmark-lab/blob/init/run-test.sh#L250-L271)\n",
    "\n",
    "References:\n",
    " - [FIO doc](https://fio.readthedocs.io/en/latest/fio_doc.html)\n",
    " - This report (notebook): reports/fio-ebs_gp3-b2.ipynb\n",
    " - This report (markdown/exported): docs/examples/fio-ebs_gp3-b2.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "! pip install pandas matplotlib natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tarfile\n",
    "from pprint import pprint\n",
    "\n",
    "from natsort import natsorted\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name_map = {\n",
    "    \"fio_ebs_initialize\": \"0_fio_ebs_initialize\",\n",
    "    \"fio_psync_randwrite\": \"1_fio_psync_randwrite\",\n",
    "    \"fio_libaio_read\": \"2_fio_libaio_read\",\n",
    "    \"fio_libaio_write\": \"3_fio_libaio_write\",\n",
    "    \"fio_libaio_rw\": \"X_fio_libaio_rw\",\n",
    "    \"fio_libaio_randread\": \"4_fio_libaio_randread\",\n",
    "    \"fio_libaio_randwrite\": \"5_fio_libaio_randwrite\",\n",
    "    \"fio_libaio_randrw\": \"6_fio_libaio_randrw\",\n",
    "    \"fio_sync_read\": \"X_fio_sync_read\",\n",
    "    \"fio_sync_write\": \"X_fio_sync_write\",\n",
    "    \"fio_sync_rw\": \"X_fio_sync_rw\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "#os.environ['JOB_GROUP'] = \"b3_loop10\"\n",
    "job_group=(f\"{os.getenv('JOB_GROUP', 'b3_loop1')}\")\n",
    "\n",
    "results_path=(f\"/results/byGroup-{job_group}\")\n",
    "parser_path = (f\"{results_path}/parser\")\n",
    "html_output_path = (f\"{parser_path}/{job_group}.html\")\n",
    "\n",
    "now = datetime.now()\n",
    "html_output = (\"<h2> FIO Benchmark Report </h2>\")\n",
    "html_output += (\"Generated at: \" + now.strftime(\"%Y-%d-%m, %H:%M:%S\") + \"UTC <br>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_add_table(html_output, title=\"\", desc=\"\", data=\"\"):\n",
    "    html_output += (f\"<br><h4>{title}</h4>\")\n",
    "    html_output += (f\"{desc}\")\n",
    "    html_output += (f\"<br><br>{data}\")\n",
    "    return html_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_result_files(base_path, results=[], start_str=\"\", contains_str=\"\", extension=\"\", ignore_str=None):\n",
    "    \"\"\"\n",
    "    Generic lookup based on filters criteria\n",
    "    \"\"\"\n",
    "    for res in os.listdir(base_path):\n",
    "        # check prefix\n",
    "        if not res.startswith(filter_results_by_battery):\n",
    "            #print(f\"01: {res}\")\n",
    "            continue\n",
    "\n",
    "        # check extension\n",
    "        if not res.endswith(extension):\n",
    "            #print(f\"02: {res}\")\n",
    "            continue\n",
    "\n",
    "        # check filter\n",
    "        if contains_str not in res:\n",
    "            #print(f\"03: {res}\")\n",
    "            continue\n",
    "\n",
    "        # ignore strings\n",
    "        if (ignore_str != None) and (ignore_str in res):\n",
    "            #print(f\"04: {res}\")\n",
    "            continue\n",
    "\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom node alias builder. To get shorter columns =)\n",
    "ocp_default_subnets = [\n",
    "    {\n",
    "        \"azName\": \"us-east-1a-public\",\n",
    "        \"azId\": \"use1-az4\",\n",
    "        \"cidr\": \"10.0.0.0/20\",\n",
    "        \"cidr_3o_start\": 0,\n",
    "        \"cidr_3o_end\": 127\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1b-public\",\n",
    "        \"azId\": \"use1-az6\",\n",
    "        \"cidr\": \"10.0.16.0/20\",\n",
    "        \"cidr_3o_start\": 16,\n",
    "        \"cidr_3o_end\": 31\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1c-public\",\n",
    "        \"azId\": \"use1-az1\",\n",
    "        \"cidr\": \"10.0.32.0/20\",\n",
    "        \"cidr_3o_start\": 32,\n",
    "        \"cidr_3o_end\": 47\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1a-private\",\n",
    "        \"azId\": \"use1-az4\",\n",
    "        \"cidr\": \"10.0.128.0/20\",\n",
    "        \"cidr_3o_start\": 128,\n",
    "        \"cidr_3o_end\": 143\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1b-private\",\n",
    "        \"azId\": \"use1-az6\",\n",
    "        \"cidr\": \"10.0.144.0/20\",\n",
    "        \"cidr_3o_start\": 144,\n",
    "        \"cidr_3o_end\": 159\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1c-private\",\n",
    "        \"azId\": \"use1-az2\",\n",
    "        \"cidr\": \"10.0.160.0/20\",\n",
    "        \"cidr_3o_start\": 160,\n",
    "        \"cidr_3o_end\": 175\n",
    "    }\n",
    "]\n",
    "\n",
    "def locate_azId_by_hostname(hostname):\n",
    "    \"\"\"\n",
    "    Assume AzId by hostname. OCP, by default, will deploy first on AzName=a and so on, with standard cidr,\n",
    "    so discovery it in us-east-1 in a standard IPI is easy;\n",
    "    \"\"\"\n",
    "    hostname_ip = (hostname.split('ip-')[1].split('.ec2.internal')[0])\n",
    "    hostname_ip3o = int((hostname_ip.split('-')[2]))\n",
    "    hostname_netIp = (f\"{int((hostname_ip.split('-')[2]))}-{int((hostname_ip.split('-')[3]))}\")\n",
    "    for net in ocp_default_subnets:\n",
    "        if (hostname_ip3o >= net['cidr_3o_start']) and (hostname_ip3o <= net['cidr_3o_end']):\n",
    "            return (net['azId'], net['azName'], hostname_netIp)\n",
    "\n",
    "    return ('AzNotFound', 'NA', hostname_netIp)\n",
    "\n",
    "\n",
    "def find_node_alias_by_hostname(hostname=\"\", add_prefix=\"\", add_suffix=\"\", fmt=\"azId\"):\n",
    "    # assuming all AWS node hostname starts with 'ip-...'\n",
    "    if hostname.startswith('ip-'):\n",
    "        azId, azName, netIp = locate_azId_by_hostname(hostname)\n",
    "        if fmt == \"azId_ipNet\": #> '{region_id}-{az_id}_{ip3o}-{ip4o}'\n",
    "            return (f\"{add_prefix}{azId}_{netIp}{add_suffix}\")\n",
    "        elif fmt == \"azIdShort_ipNet\":  #> '{az_id}_{ip3o}-{ip4o}'\n",
    "            return (f\"{add_prefix}{azId.split('-')[1]}_{netIp}{add_suffix}\")\n",
    "        elif fmt == \"ipNet_azIdShort\":  #> '{ip3o}-{ip4o}_{az_id}'\n",
    "            return (f\"{add_prefix}{netIp}_{azId.split('-')[1]}{add_suffix}\")\n",
    "        else: # \"azId\" #> '{region_id}-{az_id}'\n",
    "            return (f\"{add_prefix}{azId}{add_suffix}\")\n",
    "    \n",
    "    # default: not transformations\n",
    "    return hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, name, cluster):\n",
    "        self.node_name=name\n",
    "        self.node_alias=\"\"\n",
    "        self.cluster=cluster\n",
    "        self.cluster_full=\"\"\n",
    "\n",
    "        self.metrics=[]\n",
    "\n",
    "    def add_metric(self, **kwargs):\n",
    "        #print(f\"Adding metric [{kwargs['metric']}]\")\n",
    "        self.metrics.append({\n",
    "            \"job_name\": kwargs[\"job_name\"],\n",
    "            \"job_group\": kwargs[\"job_group\"],\n",
    "            \"task_name\": task_name_map[kwargs[\"task_name\"]],\n",
    "            \"task_group\": kwargs[\"task_group\"],\n",
    "            \"task_execId\": kwargs[\"execId\"],\n",
    "            \"timestamp\": kwargs[\"timestamp\"],\n",
    "            \"metric\": kwargs[\"metric\"],\n",
    "            \"value\": kwargs[\"value\"],\n",
    "        })\n",
    "\n",
    "\n",
    "class Nodes(object):\n",
    "    def __init__(self):\n",
    "        self.nodes={}\n",
    "    \n",
    "    def add_node(self, node, cluster):\n",
    "        try:\n",
    "            node = self.nodes[node]\n",
    "        except KeyError:\n",
    "            self.nodes[node] = Node(node, cluster)\n",
    "            print(f\"Node [{node}] added\")\n",
    "            #self.nodes[node].node_alias = find_node_alias_by_hostname(hostname=node, add_prefix=f\"{cluster}_\", fmt=\"azIdShort_ipNet\")\n",
    "            self.nodes[node].node_alias = find_node_alias_by_hostname(hostname=node, add_prefix=f\"{cluster}_\")\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "    def get_node(self, node):\n",
    "        try:\n",
    "            return self.nodes[node]\n",
    "        except:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_results_fio_runtime(node, data_path, job_info):\n",
    "    \"\"\"\n",
    "    FIO runtime log parser. See below some examples of data.\n",
    "    sample of header line:\n",
    "    #cluster=c1gp2x1> Running task [fio_psync_randwrite] on node [ip-10-0-142-138.ec2.internal], registering on log file ./.local/results/byGroup-b3_loop1/fio_stdout-c1-ip-10-0-142-138.ec2.internal.txt\n",
    "    \n",
    "    sample of metric line:\n",
    "    [0] <=> ip-10-0-142-138 <=> Thu Sep  9 13:51:16 UTC 2021 <=>  13:51:16 up 32 min,  0 users,  load average: 1.27, 0.83, 1.21 \n",
    "    [1] <=> ip-10-0-142-138 <=> Thu Sep  9 14:03:16 UTC 2021 <=>  14:03:16 up 44 min,  0 users,  load average: 0.87, 2.34, 2.93 \n",
    "    \"\"\"\n",
    "\n",
    "    job_name, job_group = job_info\n",
    "    task_group = \"fio_runtime\"\n",
    "    with open(data_path) as f:\n",
    "        last_job = ''\n",
    "        time_init = None\n",
    "        current_task = \"\"\n",
    "        for line in f.readlines():\n",
    "            # parse line : [...] Running task [fio_psync_randwrite] [...],\n",
    "            if 'Running task [' in line:\n",
    "                current_task = line.split('Running task [')[1].split(']')[0]\n",
    "                if node.cluster_full == \"\":\n",
    "                    node.cluster_full = line.split('#cluster=')[1].split('>')[0]\n",
    "                continue\n",
    "            if line.startswith('['):\n",
    "                # extract jobId, time and Load1\n",
    "                jobId = line.split(' <=> ')[0].replace('[','').replace(']','')\n",
    "                load1 = line.split(' <=> ')[3].split('load average: ')[1].split(',')[0]\n",
    "                ts = line.split(' <=> ')[2]\n",
    "                node.add_metric(job_name=job_name,\n",
    "                                job_group=job_group,\n",
    "                                task_name=current_task,\n",
    "                                task_group=task_group,\n",
    "                                execId=jobId,\n",
    "                                timestamp=ts,\n",
    "                                metric='load1',\n",
    "                                value=load1)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIO Payload (Sample)\n",
    "\n",
    "Payload sample to build the metric parser fn()\n",
    "\n",
    "> Open sample FIO result `reports/sample-fio_psync_randwrite.json` from task `fio_psync_randwrite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_results_fio_tasks(node, data_path, job_info):\n",
    "    \"\"\"\n",
    "    FIO payload parser.\n",
    "    Walk through fio result dir and load JSON files with FIO results,\n",
    "    returning only desired metrics for each test.\n",
    "    \"\"\"\n",
    "    job_name, job_group, task_name = job_info\n",
    "    task_group = \"fio_tasks\"\n",
    "\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                fpath=os.path.join(root, file)\n",
    "                with open(fpath, 'r') as f:\n",
    "                    res_payload=json.loads(f.read())\n",
    "\n",
    "                    # Extract jobId from different standards (latest is fio_io_)\n",
    "                    try:\n",
    "                        jobId = res_payload['jobs'][0]['jobname'].split('fio_io_')[1]\n",
    "                    except Exception as e:\n",
    "                        raise e\n",
    "                        \n",
    "                    #print(job_group, job_name, task_name, task_group, jobId)\n",
    "                    #pprint(res_payload)\n",
    "\n",
    "                    ts = res_payload['timestamp']\n",
    "                    metrics_collection = {\n",
    "                        \"global\": {\n",
    "                            \"read_ios\": res_payload['disk_util'][0]['read_ios'],\n",
    "                            \"write_ios\": res_payload['disk_util'][0]['read_ios'],\n",
    "                            \"bs\": res_payload['global options']['bs'],\n",
    "                            \"ioengine\": res_payload['global options']['ioengine'],\n",
    "                            \"numjobs\": res_payload['global options']['numjobs'],\n",
    "                            \"runtime\": res_payload['global options']['runtime'],\n",
    "                            \"rw\": res_payload['global options']['rw'],\n",
    "                            \"size\": res_payload['global options']['size'],\n",
    "                            \"jobname\": res_payload['jobs'][0]['jobname'],\n",
    "                        },\n",
    "                        \"values\": {\n",
    "                            \"elapsed\": res_payload['jobs'][0]['elapsed'],\n",
    "                            \"latency_ms\": res_payload['jobs'][0]['latency_ms'],\n",
    "                            \"read_bw\": res_payload['jobs'][0]['read']['bw'],\n",
    "                            \"read_iops\": res_payload['jobs'][0]['read']['iops'],\n",
    "                            \"read_total_ios\": res_payload['jobs'][0]['read']['total_ios'],\n",
    "                            \"read_lat_ms_min\": (float(res_payload['jobs'][0]['read']['lat_ns']['min'])/1e+6),\n",
    "                            \"read_lat_ms_max\": (float(res_payload['jobs'][0]['read']['lat_ns']['max'])/1e+6),\n",
    "                            \"read_lat_ms_mean\": (float(res_payload['jobs'][0]['read']['lat_ns']['mean'])/1e+6),\n",
    "                            \"read_clat_ms_p99\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"read_clat_ms_p99.9\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"read_clat_ms_p99.99\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"read_clat_ms_stddev\": (float(res_payload['jobs'][0]['read']['clat_ns']['stddev'])/1e+6),\n",
    "                            \"write_bw\": res_payload['jobs'][0]['write']['bw'],\n",
    "                            \"write_iops\": res_payload['jobs'][0]['write']['iops'],\n",
    "                            \"write_total_ios\": res_payload['jobs'][0]['write']['total_ios'],\n",
    "                            \"write_lat_ms_min\": (float(res_payload['jobs'][0]['write']['lat_ns']['min'])/1e+6),\n",
    "                            \"write_lat_ms_max\": (float(res_payload['jobs'][0]['write']['lat_ns']['max'])/1e+6),\n",
    "                            \"write_lat_ms_mean\": (float(res_payload['jobs'][0]['write']['lat_ns']['mean'])/1e+6),\n",
    "                            \"write_clat_ms_p99\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"write_clat_ms_p99.9\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"write_clat_ms_p99.99\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"write_clat_ms_stddev\": (float(res_payload['jobs'][0]['write']['clat_ns']['stddev'])/1e+6),\n",
    "                            \"sync_total_ios\": res_payload['jobs'][0]['sync']['total_ios'],\n",
    "                            \"sync_lat_ms_min\": (float(res_payload['jobs'][0]['sync']['lat_ns']['min'])/1e+6),\n",
    "                            \"sync_lat_ms_max\": (float(res_payload['jobs'][0]['sync']['lat_ns']['max'])/1e+6),\n",
    "                            \"sync_lat_ms_mean\": (float(res_payload['jobs'][0]['sync']['lat_ns']['mean'])/1e+6),\n",
    "                            \"sync_lat_ms_p99\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"sync_lat_ms_p99.9\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"sync_lat_ms_p99.99\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"sync_lat_ms_stddev\": (float(res_payload['jobs'][0]['sync']['lat_ns']['stddev'])/1e+6),\n",
    "                            \"cpu_sys\": res_payload['jobs'][0]['sys_cpu'],\n",
    "                            \"cpu_usr\": res_payload['jobs'][0]['usr_cpu'],\n",
    "                            \"cpu_ctx\": res_payload['jobs'][0]['ctx']\n",
    "                        }\n",
    "                    }\n",
    "                    node.add_metric(job_name=job_name,\n",
    "                                job_group=job_group,\n",
    "                                task_name=task_name,\n",
    "                                task_group=task_group,\n",
    "                                execId=jobId,\n",
    "                                timestamp=ts,\n",
    "                                metric='collection',\n",
    "                                value=metrics_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metric_collection(data, metric_name, is_collection=True):\n",
    "    \"\"\"\n",
    "    Filter desired {metric_name}, extract the jobs (rows) for each cluster (columns),\n",
    "    and return the data frame.\n",
    "    JobId | {cluster1}  | [...clusterN |]\n",
    "    #id   | metricValue | [...metricValue |]\n",
    "    \"\"\"\n",
    "    data_metric = {}\n",
    "    \n",
    "    def insert_metric(nid, jid, val):\n",
    "        try:\n",
    "            job = data_metric[jid]\n",
    "        except KeyError:\n",
    "            data_metric[jid] = {\n",
    "                \"job_Id\": jid\n",
    "            }\n",
    "            job = data_metric[jid]\n",
    "            pass\n",
    "        job[nid] = val\n",
    "\n",
    "    for n in data.nodes.keys():\n",
    "        node = data.nodes[n]\n",
    "        for metric in node.metrics:\n",
    "\n",
    "            job_id = (f\"{metric['task_name']}#{metric['task_execId']}\")\n",
    "            \n",
    "            # get simple metric value (metric != 'collection')\n",
    "            if not(is_collection) or (metric['metric'] != \"collection\"):\n",
    "                if metric['metric'] == metric_name:\n",
    "                    insert_metric(node.node_alias, job_id, metric['value'])\n",
    "                continue\n",
    "\n",
    "            #print(metric['metric'])\n",
    "            #print(node.node_alias, metric_name)\n",
    "            #print(metric['value']['values'])\n",
    "            insert_metric(node.node_alias, job_id, metric['value']['values'][metric_name])\n",
    "            #jid[node.node_alias] = metric['value']['values'][metric_name]\n",
    "\n",
    "    data_pd = []\n",
    "    for dk in natsorted(data_metric.keys()):\n",
    "        data_pd.append(data_metric[dk])\n",
    "\n",
    "    # create data frame and force job_id as first column\n",
    "    df = pd.read_json(json.dumps(data_pd))\n",
    "    #columns = \n",
    "    #print(columns)\n",
    "    return df.reindex(['job_Id'] + natsorted(df.columns.drop('job_Id')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_style_high(val, value_yellow=None, value_red=None, value_greenS=None, value_greenH=None, invert=False):\n",
    "    \"Data frame styling / cell formating\"\n",
    "    color_map = {\n",
    "        \"green_soft\": \"#DAF7A6\",\n",
    "        \"green_hard\": \"#02FC11\",\n",
    "        \"red_hard\": \"#FC5A5A\",\n",
    "        \"yellow_hard\": \"#E6ED02\",\n",
    "    }\n",
    "    color = None\n",
    "\n",
    "    # ignore 0 values\n",
    "    if (invert) and (val == 0.0):\n",
    "        return color\n",
    "    \n",
    "    # yellow (high)\n",
    "    if ((value_yellow != None) and not(invert)) and (val >=  value_yellow):\n",
    "        color = color_map[\"yellow_hard\"]\n",
    "    if ((value_yellow != None) and (invert)) and (val <=  value_yellow):\n",
    "        color = color_map[\"yellow_hard\"]\n",
    "    \n",
    "    # red (very high)\n",
    "    if ((value_red != None) and not(invert))  and (val >=  value_red):\n",
    "        color = color_map[\"red_hard\"]\n",
    "    if ((value_red != None) and (invert)) and (val <=  value_red):\n",
    "        color = color_map[\"red_hard\"]\n",
    "\n",
    "    # blue (low)\n",
    "    if ((value_greenS != None) and not(invert))  and (val <=  value_greenS):\n",
    "        color = color_map[\"green_soft\"]\n",
    "    if ((value_greenS != None) and (invert)) and (val >=  value_greenS):\n",
    "        color = color_map[\"green_soft\"]\n",
    "\n",
    "    # green (very low)\n",
    "    if ((value_greenH != None) and not(invert))  and (val <=  value_greenH):\n",
    "        color = color_map[\"green_hard\"]\n",
    "    if ((value_greenH != None) and (invert)) and (val >=  value_greenH):\n",
    "        color = color_map[\"green_hard\"]\n",
    "        \n",
    "    # default color\n",
    "    if color == None:\n",
    "        return color\n",
    "   \n",
    "    #return f\"color: {color}\"\n",
    "    return f\"background-color: {color}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery and Load results for 'fio' tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "#battery_id = \"b2\"\n",
    "filter_results_by_battery=\"\"\n",
    "\n",
    "nodes = {}\n",
    "\n",
    "# Runtime runtime, custom stdout collecting when FIO jobs was running\n",
    "fio_runtime = {}\n",
    "\n",
    "# FIO Runtime log parser\n",
    "result_fio_runtime_files = []\n",
    "\n",
    "# Nodes entity\n",
    "nodes = Nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fio_runtime_files = lookup_result_files(results_path,\n",
    "                                                results=result_fio_runtime_files,\n",
    "                                                start_str=filter_results_by_battery,\n",
    "                                                contains_str=\"fio_stdout\",\n",
    "                                                extension=\".txt\"\n",
    "                                               )\n",
    "len(result_fio_runtime_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_fio_runtime_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build metrics from FIO Runtime (stdout parser)\n",
    "for res in result_fio_runtime_files:\n",
    "    task_name = f\"{res.split('-')[0]}\"\n",
    "    job_name = f\"{res.split('-')[1]}\"\n",
    "    node_name = f\"{res.split(job_name+'-')[1].split('.txt')[0]}\"\n",
    "\n",
    "    nodes.add_node(node_name, job_name)\n",
    "\n",
    "    parser_results_fio_runtime(nodes.get_node(node_name), f\"{results_path}/{res}\", job_info=(job_name, job_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.nodes['ip-10-0-166-6.ec2.internal'].node_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes['ip-10-0-166-6.ec2.internal'].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIO raw payload: files is saved on the format: {battery_id}_{cluster_id}-fio-{hostname}.tar.gz ;\n",
    "# TODO unpack it, currently it should be done manually\n",
    "results_dirs_fio = []\n",
    "results_dirs_fio = lookup_result_files(results_path,\n",
    "                                        results=results_dirs_fio,\n",
    "                                        start_str=\"fio_\",\n",
    "                                        contains_str=\"fio_\",\n",
    "                                        extension=\"tar.gz\",\n",
    "                                        ignore_str=\".txt\"\n",
    "                                       )\n",
    "len(results_dirs_fio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results_dirs_fio:\n",
    "\n",
    "    task_name = f\"{res.split('-')[0]}\"\n",
    "    job_name = f\"{res.split('-')[1]}\"\n",
    "    node_name = f\"{res.split(job_name+'-')[1].split('.tar.gz')[0]}\"\n",
    "    \n",
    "    nodes.add_node(node_name, job_name)\n",
    "\n",
    "    # crate parser result dir and unpack it\n",
    "    dest_path_res = f\"{parser_path}/{res.split('.tar.gz')[0]}\"\n",
    "    \n",
    "    # dependens: mkdir .local/results/byGroup-b3_loop10/parser && chmod o+rw .local/results/byGroup-b3_loop10/parser\n",
    "    !mkdir -p f\"{dest_path_res}\"\n",
    "    \n",
    "    try:\n",
    "        if res.endswith('tar.gz'):\n",
    "            tar = tarfile.open(f\"{results_path}/{res}\")\n",
    "            tar.extractall(path=dest_path_res)\n",
    "            tar.close()\n",
    "    except:\n",
    "        # when the file is not found, or corrupted. Add empty metric\n",
    "        nodes.get_node(node_name).add_metric(\n",
    "            job_name=job_name,\n",
    "            job_group=job_group,\n",
    "            task_name=task_name,\n",
    "            task_group=\"fio_tasks\",\n",
    "            execId='',\n",
    "            timestamp='',\n",
    "            metric='empty',\n",
    "            value=''\n",
    "        )\n",
    "        print(job_group, job_name, task_name, \"fio_tasks\", node_name)\n",
    "        print(f\"ERR, dataset not found or corrupted [{res}]; Empty metric added\")\n",
    "        continue\n",
    "\n",
    "    # parser    \n",
    "    parser_results_fio_tasks(nodes.get_node(node_name), f\"{dest_path_res}\", job_info=(job_name, job_group, task_name))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes[node_name].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_fio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for 'fio'\n",
    "\n",
    "As described, the tests was done in 4 clusters in two disk layouts (single disk, etcd isolated) using gp2 and gp3. The volume has same capacity using standard values for IOPS and throughput (gp3)\n",
    "\n",
    "- Total of FIO consecutive tests: 50\n",
    "- Max IOPS on all jobs job: ~1.5/2k IOPS\n",
    "- Max IOPS for gp2 device: 386 (capacity=128GiB, throughput*=128 MiB/s)\n",
    "- Max IOPS for gp3 device: 3000 (capacity=128GiB, throughput=120MiB/s) \n",
    "\n",
    "\\*[Important note from AWS doc](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html): \n",
    "*\"The throughput limit is between 128 MiB/s and 250 MiB/s, depending on the volume size. Volumes smaller than or equal to 170 GiB deliver a maximum throughput of 128 MiB/s. Volumes larger than 170 GiB but smaller than 334 GiB deliver a maximum throughput of 250 MiB/s if burst credits are available. Volumes larger than or equal to 334 GiB deliver 250 MiB/s regardless of burst credits. gp2 volumes that were created before December 3, 2018 and that have not been modified since creation might not reach full performance unless you modify the volume.\"*\n",
    "\n",
    "____\n",
    "____\n",
    "**FIO sync lattency p99 in ms (sync_lat_p99_ms)**\n",
    "\n",
    "Summary of results:\n",
    "- after 32nd job the gp2 disks consumed all burst credits (higher than max [380 IOPS]) and become slow (5/6x) due to throttlings\n",
    "- the cluster with etcd as second disk using gp2 was more reliable for a longer period, comparing with single disk node\n",
    "- gp3 become bellow from max and stable until the end of all tests\n",
    "- gp3 in normal conditions had lattency higher than gp2\n",
    "- Trade-off in reliability (when long intensive IOPS) and performance (in normal operation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (Lattency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"lat_\",\n",
    "    \"read\": \"lat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_mean\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\"\\n>> {title}\")\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5.4, value_red=6).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=open(\"styled_dataframe.html\",\"w\")\n",
    "# f.write(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5.4, value_red=6).to_html()) # df is the styled dataframe\n",
    "# f.close()\n",
    "\n",
    "# print(df)\n",
    "# df_styled = df.style.format('{:,.2f}').set_properties(**{\n",
    "#     'font': 'Arial', 'color': 'red', 'text-align': 'center'})\n",
    "\n",
    "#display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5.4, value_red=6).set_table_attributes('border=\"1\"'))\n",
    "#dfs.set_table_attributes('border=\"1\"')\n",
    "\n",
    "#df.style.set_table_attributes('border=\"1\"')\n",
    "\n",
    "#df_styled = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5.4, value_red=6)\n",
    "#df_styled.format('{:,.2f}').set_properties(**{\n",
    "#    'font': 'Arial', 'color': 'red', 'text-align': 'center'})\n",
    "#df_styled.set_table_styles([{'props': [('background-color', 'MintCream'), ('color', 'blue'), ('font', 'Arial'), ('font-weight', 'bold')]}])\n",
    "#df_styled.set_table_attributes('border=\"2\"')\n",
    "\n",
    "\n",
    "\n",
    "#HTML(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"lat_\",\n",
    "    \"read\": \"lat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_max\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    #display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=20, value_red=50))\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=20, value_red=50).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (Percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"clat_\",\n",
    "    \"read\": \"clat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_p99\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    #display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5, value_red=10.0))\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5, value_red=10.0).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"clat_\",\n",
    "    \"read\": \"clat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_p99.9\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    #display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=10, value_red=20.0))\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=10, value_red=20.0).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"clat_\",\n",
    "    \"read\": \"clat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_stddev\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    #display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=1, value_red=2))\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=1, value_red=2).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"\",\n",
    "    \"read\": \"\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}iops\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    #display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=2000, value_red=1000, value_greenH=2950, value_greenS=2900, invert=True))\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=2000, value_red=1000, value_greenH=2950, value_greenS=2900, invert=True).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"\",\n",
    "    \"read\": \"\",\n",
    "    \"sync\": \"\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}total_ios\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    #display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=200000, value_red=100000, value_greenS=500000, value_greenH=530000, invert=True))\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=200000, value_red=100000, value_greenS=500000, value_greenH=530000, invert=True).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"\",\n",
    "    \"read\": \"\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}bw\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    #display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=20000, value_red=10000, value_greenS=40000, value_greenH=47500, invert=True))\n",
    "    dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=20000, value_red=10000, value_greenS=40000, value_greenH=47500, invert=True).set_table_attributes('border=\"1\"')\n",
    "    display(dfs)\n",
    "    html_output = output_add_table(html_output,\n",
    "                                   title=f\"{title}\",\n",
    "                                   desc=\"\",\n",
    "                                   data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_ctx\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "#display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=530000, value_red=537500))\n",
    "dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=530000, value_red=537500).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_sys\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "#display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.25, value_red=0.50, value_greenS=0.1, value_greenH=0.04))\n",
    "dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.25, value_red=0.50, value_greenS=0.1, value_greenH=0.04).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_usr\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "#display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.1, value_red=0.2, value_greenS=0.05, value_greenH=0.02))\n",
    "dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.1, value_red=0.2, value_greenS=0.05, value_greenH=0.02).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"load1\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\", is_collection=False)\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "#display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=2, value_red=4, value_greenS=1, value_greenH=0.5))\n",
    "dfs = df.style.applymap(_df_style_high, subset=df_columns, value_yellow=2, value_red=4, value_greenS=1, value_greenH=0.5).set_table_attributes('border=\"1\"')\n",
    "display(dfs)\n",
    "html_output = output_add_table(html_output,\n",
    "                               title=f\"{title}\",\n",
    "                               desc=\"\",\n",
    "                               data=dfs.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to HTML (ps: nbconvert is truncing the tables)\n",
    "with open(html_output_path, 'w') as f:\n",
    "    f.write(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
