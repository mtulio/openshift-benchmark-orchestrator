{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report FIO results for EBS Benchmark on gp2 and gp3\n",
    "\n",
    "Steps to run this report:\n",
    "\n",
    "> TODO\n",
    "\n",
    "### Description:\n",
    "\n",
    "Scenario (clusters):\n",
    "- c1: OCP cluster with 1x gp2\n",
    "- c2: OCP cluster with 2x gp2 (etcd isolated)\n",
    "- c3: OCP cluster with 1x gp3\n",
    "- c4: OCP cluster with 2x gp3 (etcd isolated)\n",
    "\n",
    "This report aggregates the data collected on FIO tests, that tested all control plane disks on layouts described above.\n",
    "\n",
    "The script to create the \"battery 2\" and collect the data is defined (by WIP script) [here](https://github.com/mtulio/openshift-cluster-benchmark-lab/blob/init/run-test.sh#L250-L271)\n",
    "\n",
    "References:\n",
    " - [FIO doc](https://fio.readthedocs.io/en/latest/fio_doc.html)\n",
    " - This report (notebook): reports/fio-ebs_gp3-b2.ipynb\n",
    " - This report (markdown/exported): docs/examples/fio-ebs_gp3-b2.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "! pip install pandas matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tarfile\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "job_group=\"b3_loop10\"\n",
    "results_path=(f\"/results/byGroup-{job_group}\")\n",
    "\n",
    "parser_path = f\"{results_path}/parser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_result_files(base_path, results=[], start_str=\"\", contains_str=\"\", extension=\"\", ignore_str=None):\n",
    "    \"\"\"\n",
    "    Generic lookup based on filters criteria\n",
    "    \"\"\"\n",
    "    for res in os.listdir(base_path):\n",
    "        # check prefix\n",
    "        if not res.startswith(filter_results_by_battery):\n",
    "            #print(f\"01: {res}\")\n",
    "            continue\n",
    "\n",
    "        # check extension\n",
    "        if not res.endswith(extension):\n",
    "            #print(f\"02: {res}\")\n",
    "            continue\n",
    "\n",
    "        # check filter\n",
    "        if contains_str not in res:\n",
    "            #print(f\"03: {res}\")\n",
    "            continue\n",
    "\n",
    "        # ignore strings\n",
    "        if (ignore_str != None) and (ignore_str in res):\n",
    "            #print(f\"04: {res}\")\n",
    "            continue\n",
    "\n",
    "        results.append(res)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom node alias builder. To get shorter columns =)\n",
    "ocp_default_subnets = [\n",
    "    {\n",
    "        \"azName\": \"us-east-1a-public\",\n",
    "        \"azId\": \"use1-az4\",\n",
    "        \"cidr\": \"10.0.0.0/20\",\n",
    "        \"cidr_3o_start\": 0,\n",
    "        \"cidr_3o_end\": 127\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1b-public\",\n",
    "        \"azId\": \"use1-az6\",\n",
    "        \"cidr\": \"10.0.16.0/20\",\n",
    "        \"cidr_3o_start\": 16,\n",
    "        \"cidr_3o_end\": 31\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1c-public\",\n",
    "        \"azId\": \"use1-az1\",\n",
    "        \"cidr\": \"10.0.32.0/20\",\n",
    "        \"cidr_3o_start\": 32,\n",
    "        \"cidr_3o_end\": 47\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1a-private\",\n",
    "        \"azId\": \"use1-az4\",\n",
    "        \"cidr\": \"10.0.128.0/20\",\n",
    "        \"cidr_3o_start\": 128,\n",
    "        \"cidr_3o_end\": 143\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1b-private\",\n",
    "        \"azId\": \"use1-az6\",\n",
    "        \"cidr\": \"10.0.144.0/20\",\n",
    "        \"cidr_3o_start\": 144,\n",
    "        \"cidr_3o_end\": 159\n",
    "    },\n",
    "    {\n",
    "        \"azName\": \"us-east-1c-private\",\n",
    "        \"azId\": \"use1-az2\",\n",
    "        \"cidr\": \"10.0.160.0/20\",\n",
    "        \"cidr_3o_start\": 160,\n",
    "        \"cidr_3o_end\": 175\n",
    "    }\n",
    "]\n",
    "\n",
    "def locate_azId_by_hostname(hostname):\n",
    "    \"\"\"\n",
    "    Assume AzId by hostname. OCP, by default, will deploy first on AzName=a and so on, with standard cidr,\n",
    "    so discovery it in us-east-1 in a standard IPI is easy;\n",
    "    \"\"\"\n",
    "    hostname_ip = (hostname.split('ip-')[1].split('.ec2.internal')[0])\n",
    "    hostname_ip3o = int((hostname_ip.split('-')[2]))\n",
    "    hostname_netIp = (f\"{int((hostname_ip.split('-')[2]))}-{int((hostname_ip.split('-')[3]))}\")\n",
    "    for net in ocp_default_subnets:\n",
    "        if (hostname_ip3o >= net['cidr_3o_start']) and (hostname_ip3o <= net['cidr_3o_end']):\n",
    "            return (net['azId'], net['azName'], hostname_netIp)\n",
    "\n",
    "    return ('AzNotFound', 'NA', hostname_netIp)\n",
    "\n",
    "\n",
    "def find_node_alias_by_hostname(hostname=\"\", add_prefix=\"\", add_suffix=\"\", fmt=\"azId\"):\n",
    "    # assuming all AWS node hostname starts with 'ip-...'\n",
    "    if hostname.startswith('ip-'):\n",
    "        azId, azName, netIp = locate_azId_by_hostname(hostname)\n",
    "        if fmt == \"azId_ipNet\": #> '{region_id}-{az_id}_{ip3o}-{ip4o}'\n",
    "            return (f\"{add_prefix}{azId}_{netIp}{add_suffix}\")\n",
    "        elif fmt == \"azIdShort_ipNet\":  #> '{az_id}_{ip3o}-{ip4o}'\n",
    "            return (f\"{add_prefix}{azId.split('-')[1]}_{netIp}{add_suffix}\")\n",
    "        elif fmt == \"ipNet_azIdShort\":  #> '{ip3o}-{ip4o}_{az_id}'\n",
    "            return (f\"{add_prefix}{netIp}_{azId.split('-')[1]}{add_suffix}\")\n",
    "        else: # \"azId\" #> '{region_id}-{az_id}'\n",
    "            return (f\"{add_prefix}{azId}{add_suffix}\")\n",
    "    \n",
    "    # default: not transformations\n",
    "    return hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, name, cluster):\n",
    "        self.node_name=name\n",
    "        self.node_alias=\"\"\n",
    "        self.cluster=cluster\n",
    "        self.cluster_full=\"\"\n",
    "\n",
    "        self.metrics=[]\n",
    "\n",
    "    def add_metric(self, **kwargs):\n",
    "        #print(f\"Adding metric [{kwargs['metric']}]\")\n",
    "        self.metrics.append({\n",
    "            \"job_name\": kwargs[\"job_name\"],\n",
    "            \"job_group\": kwargs[\"job_group\"],\n",
    "            \"task_name\": kwargs[\"task_name\"],\n",
    "            \"task_group\": kwargs[\"task_group\"],\n",
    "            \"task_execId\": kwargs[\"execId\"],\n",
    "            \"timestamp\": kwargs[\"timestamp\"],\n",
    "            \"metric\": kwargs[\"metric\"],\n",
    "            \"value\": kwargs[\"value\"],\n",
    "        })\n",
    "\n",
    "\n",
    "class Nodes(object):\n",
    "    def __init__(self):\n",
    "        self.nodes={}\n",
    "    \n",
    "    def add_node(self, node, cluster):\n",
    "        try:\n",
    "            node = self.nodes[node]\n",
    "        except KeyError:\n",
    "            self.nodes[node] = Node(node, cluster)\n",
    "            print(f\"Node [{node}] added\")\n",
    "            #self.nodes[node].node_alias = find_node_alias_by_hostname(hostname=node, add_prefix=f\"{cluster}_\", fmt=\"azIdShort_ipNet\")\n",
    "            self.nodes[node].node_alias = find_node_alias_by_hostname(hostname=node, add_prefix=f\"{cluster}_\")\n",
    "        except:\n",
    "            raise\n",
    "\n",
    "    def get_node(self, node):\n",
    "        try:\n",
    "            return self.nodes[node]\n",
    "        except:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_results_fio_runtime(node, data_path, job_info):\n",
    "    \"\"\"\n",
    "    FIO runtime log parser. See below some examples of data.\n",
    "    sample of header line:\n",
    "    #cluster=c1gp2x1> Running task [fio_psync_randwrite] on node [ip-10-0-142-138.ec2.internal], registering on log file ./.local/results/byGroup-b3_loop1/fio_stdout-c1-ip-10-0-142-138.ec2.internal.txt\n",
    "    \n",
    "    sample of metric line:\n",
    "    [0] <=> ip-10-0-142-138 <=> Thu Sep  9 13:51:16 UTC 2021 <=>  13:51:16 up 32 min,  0 users,  load average: 1.27, 0.83, 1.21 \n",
    "    [1] <=> ip-10-0-142-138 <=> Thu Sep  9 14:03:16 UTC 2021 <=>  14:03:16 up 44 min,  0 users,  load average: 0.87, 2.34, 2.93 \n",
    "    \"\"\"\n",
    "\n",
    "    job_name, job_group = job_info\n",
    "    task_group = \"fio_runtime\"\n",
    "    with open(data_path) as f:\n",
    "        last_job = ''\n",
    "        time_init = None\n",
    "        current_task = \"\"\n",
    "        for line in f.readlines():\n",
    "            # parse line : [...] Running task [fio_psync_randwrite] [...],\n",
    "            if 'Running task [' in line:\n",
    "                current_task = line.split('Running task [')[1].split(']')[0]\n",
    "                if node.cluster_full == \"\":\n",
    "                    node.cluster_full = line.split('#cluster=')[1].split('>')[0]\n",
    "                continue\n",
    "            if line.startswith('['):\n",
    "                # extract jobId, time and Load1\n",
    "                jobId = line.split(' <=> ')[0].replace('[','').replace(']','')\n",
    "                load1 = line.split(' <=> ')[3].split('load average: ')[1].split(',')[0]\n",
    "                ts = line.split(' <=> ')[2]\n",
    "                node.add_metric(job_name=job_name,\n",
    "                                job_group=job_group,\n",
    "                                task_name=current_task,\n",
    "                                task_group=task_group,\n",
    "                                execId=jobId,\n",
    "                                timestamp=ts,\n",
    "                                metric='load1',\n",
    "                                value=load1)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIO Payload (Sample)\n",
    "\n",
    "Payload sample to build the metric parser fn()\n",
    "\n",
    "> From task result `fio_psync_randwrite`\n",
    "\n",
    "\n",
    "```json\n",
    "{'disk_util': [{'in_queue': 3112979,\n",
    "                'name': 'nvme1n1',\n",
    "                'read_ios': 3,\n",
    "                'read_merges': 0,\n",
    "                'read_ticks': 2,\n",
    "                'util': 99.986104,\n",
    "                'write_ios': 570382,\n",
    "                'write_merges': 1005,\n",
    "                'write_ticks': 3112977}],\n",
    " 'fio version': 'fio-3.6',\n",
    " 'global options': {'bs': '16k',\n",
    "                    'direct': '1',\n",
    "                    'directory': '/var/lib/etcd/_benchmark',\n",
    "                    'ioengine': 'psync',\n",
    "                    'numjobs': '16',\n",
    "                    'runtime': '180',\n",
    "                    'rw': 'randwrite',\n",
    "                    'size': '1G'},\n",
    " 'jobs': [{'ctx': 532537,\n",
    "           'elapsed': 181,\n",
    "           'error': 0,\n",
    "           'eta': 0,\n",
    "           'groupid': 0,\n",
    "           'iodepth_level': {'1': 100.0,\n",
    "                             '16': 0.0,\n",
    "                             '2': 0.0,\n",
    "                             '32': 0.0,\n",
    "                             '4': 0.0,\n",
    "                             '8': 0.0,\n",
    "                             '>=64': 0.0},\n",
    "           'job options': {'name': 'fio_io_1'},\n",
    "           'jobname': 'fio_io_1',\n",
    "           'latency_depth': 1,\n",
    "           'latency_ms': {'10': 98.602706,\n",
    "                          '100': 0.0,\n",
    "                          '1000': 0.0,\n",
    "                          '2': 0.702802,\n",
    "                          '20': 0.390593,\n",
    "                          '2000': 0.0,\n",
    "                          '250': 0.0,\n",
    "                          '4': 0.23666,\n",
    "                          '50': 0.01,\n",
    "                          '500': 0.0,\n",
    "                          '750': 0.0,\n",
    "                          '>=2000': 0.0},\n",
    "           'latency_ns': {'10': 0.0,\n",
    "                          '100': 0.0,\n",
    "                          '1000': 0.0,\n",
    "                          '2': 0.0,\n",
    "                          '20': 0.0,\n",
    "                          '250': 0.0,\n",
    "                          '4': 0.0,\n",
    "                          '50': 0.0,\n",
    "                          '500': 0.0,\n",
    "                          '750': 0.0},\n",
    "           'latency_percentile': 100.0,\n",
    "           'latency_target': 0,\n",
    "           'latency_us': {'10': 0.0,\n",
    "                          '100': 0.0,\n",
    "                          '1000': 0.064028,\n",
    "                          '2': 0.0,\n",
    "                          '20': 0.0,\n",
    "                          '250': 0.0,\n",
    "                          '4': 0.0,\n",
    "                          '50': 0.0,\n",
    "                          '500': 0.0,\n",
    "                          '750': 0.01},\n",
    "           'latency_window': 0,\n",
    "           'majf': 0,\n",
    "           'minf': 143,\n",
    "           'read': {'bw': 0,\n",
    "                    'bw_agg': 0.0,\n",
    "                    'bw_bytes': 0,\n",
    "                    'bw_dev': 0.0,\n",
    "                    'bw_max': 0,\n",
    "                    'bw_mean': 0.0,\n",
    "                    'bw_min': 0,\n",
    "                    'bw_samples': 0,\n",
    "                    'clat_ns': {'max': 0,\n",
    "                                'mean': 0.0,\n",
    "                                'min': 0,\n",
    "                                'percentile': {'1.000000': 0,\n",
    "                                               '10.000000': 0,\n",
    "                                               '20.000000': 0,\n",
    "                                               '30.000000': 0,\n",
    "                                               '40.000000': 0,\n",
    "                                               '5.000000': 0,\n",
    "                                               '50.000000': 0,\n",
    "                                               '60.000000': 0,\n",
    "                                               '70.000000': 0,\n",
    "                                               '80.000000': 0,\n",
    "                                               '90.000000': 0,\n",
    "                                               '95.000000': 0,\n",
    "                                               '99.000000': 0,\n",
    "                                               '99.500000': 0,\n",
    "                                               '99.900000': 0,\n",
    "                                               '99.950000': 0,\n",
    "                                               '99.990000': 0},\n",
    "                                'stddev': 0.0},\n",
    "                    'drop_ios': 0,\n",
    "                    'io_bytes': 0,\n",
    "                    'io_kbytes': 0,\n",
    "                    'iops': 0.0,\n",
    "                    'iops_max': 0,\n",
    "                    'iops_mean': 0.0,\n",
    "                    'iops_min': 0,\n",
    "                    'iops_samples': 0,\n",
    "                    'iops_stddev': 0.0,\n",
    "                    'lat_ns': {'max': 0, 'mean': 0.0, 'min': 0, 'stddev': 0.0},\n",
    "                    'runtime': 0,\n",
    "                    'short_ios': 0,\n",
    "                    'slat_ns': {'max': 0, 'mean': 0.0, 'min': 0, 'stddev': 0.0},\n",
    "                    'total_ios': 0},\n",
    "           'sync': {'lat_ns': {'max': 0,\n",
    "                               'mean': 0.0,\n",
    "                               'min': 0,\n",
    "                               'percentile': {'1.000000': 0,\n",
    "                                              '10.000000': 0,\n",
    "                                              '20.000000': 0,\n",
    "                                              '30.000000': 0,\n",
    "                                              '40.000000': 0,\n",
    "                                              '5.000000': 0,\n",
    "                                              '50.000000': 0,\n",
    "                                              '60.000000': 0,\n",
    "                                              '70.000000': 0,\n",
    "                                              '80.000000': 0,\n",
    "                                              '90.000000': 0,\n",
    "                                              '95.000000': 0,\n",
    "                                              '99.000000': 0,\n",
    "                                              '99.500000': 0,\n",
    "                                              '99.900000': 0,\n",
    "                                              '99.950000': 0,\n",
    "                                              '99.990000': 0},\n",
    "                               'stddev': 0.0},\n",
    "                    'total_ios': 0},\n",
    "           'sys_cpu': 0.408606,\n",
    "           'trim': {'bw': 0,\n",
    "                    'bw_agg': 0.0,\n",
    "                    'bw_bytes': 0,\n",
    "                    'bw_dev': 0.0,\n",
    "                    'bw_max': 0,\n",
    "                    'bw_mean': 0.0,\n",
    "                    'bw_min': 0,\n",
    "                    'bw_samples': 0,\n",
    "                    'clat_ns': {'max': 0,\n",
    "                                'mean': 0.0,\n",
    "                                'min': 0,\n",
    "                                'percentile': {'1.000000': 0,\n",
    "                                               '10.000000': 0,\n",
    "                                               '20.000000': 0,\n",
    "                                               '30.000000': 0,\n",
    "                                               '40.000000': 0,\n",
    "                                               '5.000000': 0,\n",
    "                                               '50.000000': 0,\n",
    "                                               '60.000000': 0,\n",
    "                                               '70.000000': 0,\n",
    "                                               '80.000000': 0,\n",
    "                                               '90.000000': 0,\n",
    "                                               '95.000000': 0,\n",
    "                                               '99.000000': 0,\n",
    "                                               '99.500000': 0,\n",
    "                                               '99.900000': 0,\n",
    "                                               '99.950000': 0,\n",
    "                                               '99.990000': 0},\n",
    "                                'stddev': 0.0},\n",
    "                    'drop_ios': 0,\n",
    "                    'io_bytes': 0,\n",
    "                    'io_kbytes': 0,\n",
    "                    'iops': 0.0,\n",
    "                    'iops_max': 0,\n",
    "                    'iops_mean': 0.0,\n",
    "                    'iops_min': 0,\n",
    "                    'iops_samples': 0,\n",
    "                    'iops_stddev': 0.0,\n",
    "                    'lat_ns': {'max': 0, 'mean': 0.0, 'min': 0, 'stddev': 0.0},\n",
    "                    'runtime': 0,\n",
    "                    'short_ios': 0,\n",
    "                    'slat_ns': {'max': 0, 'mean': 0.0, 'min': 0, 'stddev': 0.0},\n",
    "                    'total_ios': 0},\n",
    "           'usr_cpu': 0.086839,\n",
    "           'write': {'bw': 47061,\n",
    "                     'bw_agg': 6.249132,\n",
    "                     'bw_bytes': 48190558,\n",
    "                     'bw_dev': 339.839869,\n",
    "                     'bw_max': 9184,\n",
    "                     'bw_mean': 2940.903786,\n",
    "                     'bw_min': 1760,\n",
    "                     'bw_samples': 5758,\n",
    "                     'clat_ns': {'max': 48851113,\n",
    "                                 'mean': 5437348.329637,\n",
    "                                 'min': 665825,\n",
    "                                 'percentile': {'1.000000': 3981312,\n",
    "                                                '10.000000': 5079040,\n",
    "                                                '20.000000': 5210112,\n",
    "                                                '30.000000': 5275648,\n",
    "                                                '40.000000': 5341184,\n",
    "                                                '5.000000': 4947968,\n",
    "                                                '50.000000': 5341184,\n",
    "                                                '60.000000': 5406720,\n",
    "                                                '70.000000': 5472256,\n",
    "                                                '80.000000': 5603328,\n",
    "                                                '90.000000': 5865472,\n",
    "                                                '95.000000': 6193152,\n",
    "                                                '99.000000': 7307264,\n",
    "                                                '99.500000': 9895936,\n",
    "                                                '99.900000': 12386304,\n",
    "                                                '99.950000': 12910592,\n",
    "                                                '99.990000': 14745600},\n",
    "                                 'stddev': 693740.770945},\n",
    "                     'drop_ios': 0,\n",
    "                     'io_bytes': 8674541568,\n",
    "                     'io_kbytes': 8471232,\n",
    "                     'iops': 2941.318297,\n",
    "                     'iops_max': 574,\n",
    "                     'iops_mean': 183.771101,\n",
    "                     'iops_min': 110,\n",
    "                     'iops_samples': 5758,\n",
    "                     'iops_stddev': 21.243864,\n",
    "                     'lat_ns': {'max': 48851490,\n",
    "                                'mean': 5438129.180468,\n",
    "                                'min': 667004,\n",
    "                                'stddev': 693735.52613},\n",
    "                     'runtime': 180005,\n",
    "                     'short_ios': 0,\n",
    "                     'slat_ns': {'max': 0,\n",
    "                                 'mean': 0.0,\n",
    "                                 'min': 0,\n",
    "                                 'stddev': 0.0},\n",
    "                     'total_ios': 529452}}],\n",
    " 'time': 'Thu Sep  9 12:29:38 2021',\n",
    " 'timestamp': 1631190578,\n",
    " 'timestamp_ms': 1631190578550}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_results_fio_tasks(node, data_path, job_info):\n",
    "    \"\"\"\n",
    "    FIO payload parser.\n",
    "    Walk through fio result dir and load JSON files with FIO results,\n",
    "    returning only desired metrics for each test.\n",
    "    \"\"\"\n",
    "    job_name, job_group, task_name = job_info\n",
    "    task_group = \"fio_tasks\"\n",
    "\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                fpath=os.path.join(root, file)\n",
    "                with open(fpath, 'r') as f:\n",
    "                    res_payload=json.loads(f.read())\n",
    "\n",
    "                    # Extract jobId from different standards (latest is fio_io_)\n",
    "                    try:\n",
    "                        jobId = res_payload['jobs'][0]['jobname'].split('fio_io_')[1]\n",
    "                    except Exception as e:\n",
    "                        raise e\n",
    "                        \n",
    "                    print(job_group, job_name, task_name, task_group, jobId)\n",
    "                    #pprint(res_payload)\n",
    "\n",
    "                    ts = res_payload['timestamp']\n",
    "                    metrics_collection = {\n",
    "                        \"global\": {\n",
    "                            \"read_ios\": res_payload['disk_util'][0]['read_ios'],\n",
    "                            \"write_ios\": res_payload['disk_util'][0]['read_ios'],\n",
    "                            \"bs\": res_payload['global options']['bs'],\n",
    "                            \"ioengine\": res_payload['global options']['ioengine'],\n",
    "                            \"numjobs\": res_payload['global options']['numjobs'],\n",
    "                            \"runtime\": res_payload['global options']['runtime'],\n",
    "                            \"rw\": res_payload['global options']['rw'],\n",
    "                            \"size\": res_payload['global options']['size'],\n",
    "                            \"jobname\": res_payload['jobs'][0]['jobname'],\n",
    "                        },\n",
    "                        \"values\": {\n",
    "                            \"elapsed\": res_payload['jobs'][0]['elapsed'],\n",
    "                            \"latency_ms\": res_payload['jobs'][0]['latency_ms'],\n",
    "                            \"read_bw\": res_payload['jobs'][0]['read']['bw'],\n",
    "                            \"read_iops\": res_payload['jobs'][0]['read']['iops'],\n",
    "                            \"read_total_ios\": res_payload['jobs'][0]['read']['total_ios'],\n",
    "                            \"read_lat_ms_min\": (float(res_payload['jobs'][0]['read']['lat_ns']['min'])/1e+6),\n",
    "                            \"read_lat_ms_max\": (float(res_payload['jobs'][0]['read']['lat_ns']['max'])/1e+6),\n",
    "                            \"read_lat_ms_mean\": (float(res_payload['jobs'][0]['read']['lat_ns']['mean'])/1e+6),\n",
    "                            \"read_clat_ms_p99\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"read_clat_ms_p99.9\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"read_clat_ms_p99.99\": (float(res_payload['jobs'][0]['read']['clat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"read_clat_ms_stddev\": (float(res_payload['jobs'][0]['read']['clat_ns']['stddev'])/1e+6),\n",
    "                            \"write_bw\": res_payload['jobs'][0]['write']['bw'],\n",
    "                            \"write_iops\": res_payload['jobs'][0]['write']['iops'],\n",
    "                            \"write_total_ios\": res_payload['jobs'][0]['write']['total_ios'],\n",
    "                            \"write_lat_ms_min\": (float(res_payload['jobs'][0]['write']['lat_ns']['min'])/1e+6),\n",
    "                            \"write_lat_ms_max\": (float(res_payload['jobs'][0]['write']['lat_ns']['max'])/1e+6),\n",
    "                            \"write_lat_ms_mean\": (float(res_payload['jobs'][0]['write']['lat_ns']['mean'])/1e+6),\n",
    "                            \"write_clat_ms_p99\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"write_clat_ms_p99.9\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"write_clat_ms_p99.99\": (float(res_payload['jobs'][0]['write']['clat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"write_clat_ms_stddev\": (float(res_payload['jobs'][0]['write']['clat_ns']['stddev'])/1e+6),\n",
    "                            \"sync_total_ios\": res_payload['jobs'][0]['sync']['total_ios'],\n",
    "                            \"sync_lat_ms_min\": (float(res_payload['jobs'][0]['sync']['lat_ns']['min'])/1e+6),\n",
    "                            \"sync_lat_ms_max\": (float(res_payload['jobs'][0]['sync']['lat_ns']['max'])/1e+6),\n",
    "                            \"sync_lat_ms_mean\": (float(res_payload['jobs'][0]['sync']['lat_ns']['mean'])/1e+6),\n",
    "                            \"sync_lat_ms_p99\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.000000'])/1e+6),\n",
    "                            \"sync_lat_ms_p99.9\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.900000'])/1e+6),\n",
    "                            \"sync_lat_ms_p99.99\": (float(res_payload['jobs'][0]['sync']['lat_ns']['percentile']['99.990000'])/1e+6),\n",
    "                            \"sync_lat_ms_stddev\": (float(res_payload['jobs'][0]['sync']['lat_ns']['stddev'])/1e+6),\n",
    "                            \"cpu_sys\": res_payload['jobs'][0]['sys_cpu'],\n",
    "                            \"cpu_usr\": res_payload['jobs'][0]['usr_cpu'],\n",
    "                            \"cpu_ctx\": res_payload['jobs'][0]['ctx']\n",
    "                        }\n",
    "                    }\n",
    "                    node.add_metric(job_name=job_name,\n",
    "                                job_group=job_group,\n",
    "                                task_name=task_name,\n",
    "                                task_group=task_group,\n",
    "                                execId=jobId,\n",
    "                                timestamp=ts,\n",
    "                                metric='collection',\n",
    "                                value=metrics_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_metric_collection(data, metric_name, is_collection=True):\n",
    "    \"\"\"\n",
    "    Filter desired {metric_name}, extract the jobs (rows) for each cluster (columns),\n",
    "    and return the data frame.\n",
    "    JobId | {cluster1}  | [...clusterN |]\n",
    "    #id   | metricValue | [...metricValue |]\n",
    "    \"\"\"\n",
    "\n",
    "    data_metric = {}\n",
    "    for n in data.nodes.keys():\n",
    "        node = data.nodes[n]\n",
    "        for metric in node.metrics:\n",
    "            \n",
    "\n",
    "            job_id = (f\"{metric['task_name']}#{metric['task_execId']}\")\n",
    "            try:\n",
    "                jid = data_metric[job_id]\n",
    "            except KeyError:\n",
    "                data_metric[job_id] = {\n",
    "                    \"job_Id\": job_id\n",
    "                }\n",
    "                jid = data_metric[job_id]\n",
    "                pass\n",
    "\n",
    "            if not(is_collection) or (metric['metric'] != \"collection\"):\n",
    "                if metric['metric'] == metric_name:\n",
    "                    jid[node.node_alias] = metric['value']\n",
    "                continue\n",
    "            #print(metric['metric'])\n",
    "            #print(node.node_alias, metric_name)\n",
    "            #print(metric['value']['values'])\n",
    "            jid[node.node_alias] = metric['value']['values'][metric_name]\n",
    "\n",
    "    data_pd = []\n",
    "    for dk in data_metric.keys():\n",
    "        data_pd.append(data_metric[dk])\n",
    "\n",
    "    # create data frame and force job_id as first column\n",
    "    df = pd.read_json(json.dumps(data_pd))\n",
    "    columns = df.columns.drop('job_Id')\n",
    "    return df.reindex(['job_Id'] + sorted(columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _df_style_high(val, value_yellow=None, value_red=None, value_greenS=None, value_greenH=None, invert=False):\n",
    "    \"Data frame styling / cell formating\"\n",
    "    color_map = {\n",
    "        \"green_soft\": \"#DAF7A6\",\n",
    "        \"green_hard\": \"#02FC11\",\n",
    "        \"red_hard\": \"#FC5A5A\",\n",
    "        \"yellow_hard\": \"#E6ED02\",\n",
    "    }\n",
    "    color = None\n",
    "\n",
    "    # ignore 0 values\n",
    "    if (invert) and (val == 0.0):\n",
    "        return color\n",
    "    \n",
    "    # yellow (high)\n",
    "    if ((value_yellow != None) and not(invert)) and (val >=  value_yellow):\n",
    "        color = color_map[\"yellow_hard\"]\n",
    "    if ((value_yellow != None) and (invert)) and (val <=  value_yellow):\n",
    "        color = color_map[\"yellow_hard\"]\n",
    "    \n",
    "    # red (very high)\n",
    "    if ((value_red != None) and not(invert))  and (val >=  value_red):\n",
    "        color = color_map[\"red_hard\"]\n",
    "    if ((value_red != None) and (invert)) and (val <=  value_red):\n",
    "        color = color_map[\"red_hard\"]\n",
    "\n",
    "    # blue (low)\n",
    "    if ((value_greenS != None) and not(invert))  and (val <=  value_greenS):\n",
    "        color = color_map[\"green_soft\"]\n",
    "    if ((value_greenS != None) and (invert)) and (val >=  value_greenS):\n",
    "        color = color_map[\"green_soft\"]\n",
    "\n",
    "    # green (very low)\n",
    "    if ((value_greenH != None) and not(invert))  and (val <=  value_greenH):\n",
    "        color = color_map[\"green_hard\"]\n",
    "    if ((value_greenH != None) and (invert)) and (val >=  value_greenH):\n",
    "        color = color_map[\"green_hard\"]\n",
    "        \n",
    "    # default color\n",
    "    if color == None:\n",
    "        return color\n",
    "   \n",
    "    #return f\"color: {color}\"\n",
    "    return f\"background-color: {color}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovery and Load results for 'fio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "#battery_id = \"b2\"\n",
    "filter_results_by_battery=\"\"\n",
    "\n",
    "nodes = {}\n",
    "\n",
    "# Runtime runtime, custom stdout collecting when FIO jobs was running\n",
    "fio_runtime = {}\n",
    "\n",
    "# FIO Runtime log parser\n",
    "result_fio_runtime_files = []\n",
    "\n",
    "# Nodes entity\n",
    "nodes = Nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fio_runtime_files = lookup_result_files(results_path,\n",
    "                                                results=result_fio_runtime_files,\n",
    "                                                start_str=filter_results_by_battery,\n",
    "                                                contains_str=\"fio_stdout\",\n",
    "                                                extension=\".txt\"\n",
    "                                               )\n",
    "len(result_fio_runtime_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fio_runtime_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build metrics from FIO Runtime (stdout parser)\n",
    "for res in result_fio_runtime_files:\n",
    "    task_name = f\"{res.split('-')[0]}\"\n",
    "    job_name = f\"{res.split('-')[1]}\"\n",
    "    node_name = f\"{res.split(job_name+'-')[1].split('.txt')[0]}\"\n",
    "\n",
    "    nodes.add_node(node_name, job_name)\n",
    "\n",
    "    parser_results_fio_runtime(nodes.get_node(node_name), f\"{results_path}/{res}\", job_info=(job_name, job_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.nodes['ip-10-0-166-6.ec2.internal'].node_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes['ip-10-0-166-6.ec2.internal'].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIO raw payload: files is saved on the format: {battery_id}_{cluster_id}-fio-{hostname}.tar.gz ;\n",
    "# TODO unpack it, currently it should be done manually\n",
    "results_dirs_fio = []\n",
    "results_dirs_fio = lookup_result_files(results_path,\n",
    "                                        results=results_dirs_fio,\n",
    "                                        start_str=\"fio_\",\n",
    "                                        contains_str=\"fio_\",\n",
    "                                        extension=\"tar.gz\",\n",
    "                                        ignore_str=\".txt\"\n",
    "                                       )\n",
    "results_dirs_fio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for res in results_dirs_fio:\n",
    "\n",
    "    task_name = f\"{res.split('-')[0]}\"\n",
    "    job_name = f\"{res.split('-')[1]}\"\n",
    "    node_name = f\"{res.split(job_name+'-')[1].split('.tar.gz')[0]}\"\n",
    "    \n",
    "    nodes.add_node(node_name, job_name)\n",
    "\n",
    "    # crate parser result dir and unpack it\n",
    "    dest_path_res = f\"{parser_path}/{res.split('.tar.gz')[0]}\"\n",
    "    \n",
    "    # dependens: mkdir .local/results/byGroup-b3_loop10/parser && chmod o+rw .local/results/byGroup-b3_loop10/parser\n",
    "    !mkdir -p f\"{dest_path_res}\"\n",
    "    \n",
    "    try:\n",
    "        if res.endswith('tar.gz'):\n",
    "            tar = tarfile.open(f\"{results_path}/{res}\")\n",
    "            tar.extractall(path=dest_path_res)\n",
    "            tar.close()\n",
    "    except:\n",
    "        # when the file is not found, or corrupted. Add empty metric\n",
    "        nodes.get_node(node_name).add_metric(\n",
    "            job_name=job_name,\n",
    "            job_group=job_group,\n",
    "            task_name=task_name,\n",
    "            task_group=\"fio_tasks\",\n",
    "            execId='',\n",
    "            timestamp='',\n",
    "            metric='empty',\n",
    "            value=''\n",
    "        )\n",
    "        print(job_group, job_name, task_name, \"fio_tasks\", node_name)\n",
    "        print(f\"ERR, dataset not found or corrupted [{res}]; Empty metric added\")\n",
    "        continue\n",
    "\n",
    "    # parser    \n",
    "    parser_results_fio_tasks(nodes.get_node(node_name), f\"{dest_path_res}\", job_info=(job_name, job_group, task_name))\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes[node_name].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results_fio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for 'fio'\n",
    "\n",
    "As described, the tests was done in 4 clusters in two disk layouts (single disk, etcd isolated) using gp2 and gp3. The volume has same capacity using standard values for IOPS and throughput (gp3)\n",
    "\n",
    "- Total of FIO consecutive tests: 50\n",
    "- Max IOPS on all jobs job: ~1.5/2k IOPS\n",
    "- Max IOPS for gp2 device: 386 (capacity=128GiB, throughput*=128 MiB/s)\n",
    "- Max IOPS for gp3 device: 3000 (capacity=128GiB, throughput=120MiB/s) \n",
    "\n",
    "\\*[Important note from AWS doc](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volume-types.html): \n",
    "*\"The throughput limit is between 128 MiB/s and 250 MiB/s, depending on the volume size. Volumes smaller than or equal to 170 GiB deliver a maximum throughput of 128 MiB/s. Volumes larger than 170 GiB but smaller than 334 GiB deliver a maximum throughput of 250 MiB/s if burst credits are available. Volumes larger than or equal to 334 GiB deliver 250 MiB/s regardless of burst credits. gp2 volumes that were created before December 3, 2018 and that have not been modified since creation might not reach full performance unless you modify the volume.\"*\n",
    "\n",
    "____\n",
    "____\n",
    "**FIO sync lattency p99 in ms (sync_lat_p99_ms)**\n",
    "\n",
    "Summary of results:\n",
    "- after 32nd job the gp2 disks consumed all burst credits (higher than max [380 IOPS]) and become slow (5/6x) due to throttlings\n",
    "- the cluster with etcd as second disk using gp2 was more reliable for a longer period, comparing with single disk node\n",
    "- gp3 become bellow from max and stable until the end of all tests\n",
    "- gp3 in normal conditions had lattency higher than gp2\n",
    "- Trade-off in reliability (when long intensive IOPS) and performance (in normal operation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (Lattency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes.nodes['ip-10-0-140-177.ec2.internal'].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"lat_\",\n",
    "    \"read\": \"lat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_mean\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5.4, value_red=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"lat_\",\n",
    "    \"read\": \"lat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_max\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=20, value_red=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (Percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"clat_\",\n",
    "    \"read\": \"clat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_p99\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=5, value_red=10.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"clat_\",\n",
    "    \"read\": \"clat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_p99.9\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=10, value_red=20.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"clat_\",\n",
    "    \"read\": \"clat_\",\n",
    "    \"sync\": \"lat_\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}ms_stddev\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=1, value_red=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"\",\n",
    "    \"read\": \"\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}iops\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=2000, value_red=1000, value_greenH=2950, value_greenS=2900, invert=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"\",\n",
    "    \"read\": \"\",\n",
    "    \"sync\": \"\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}total_ios\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=200000, value_red=100000, value_greenS=500000, value_greenH=530000, invert=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oper_lat_prefix = {\n",
    "    \"write\": \"\",\n",
    "    \"read\": \"\"\n",
    "}\n",
    "for op in oper_lat_prefix.keys():\n",
    "    metric=f\"{op}_{oper_lat_prefix[op]}bw\"\n",
    "    title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "    df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "    df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "    print(f\">> {title}\")\n",
    "    display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=20000, value_red=10000, value_greenS=40000, value_greenH=47500, invert=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_ctx\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=530000, value_red=537500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_sys\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.25, value_red=0.50, value_greenS=0.1, value_greenH=0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"cpu_usr\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\")\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=0.1, value_red=0.2, value_greenS=0.05, value_greenH=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=\"load1\"\n",
    "title=f\"metric ({metric}) by Node(all)\"\n",
    "\n",
    "df = aggregate_metric_collection(nodes, f\"{metric}\", is_collection=False)\n",
    "df_columns = df.columns.drop('job_Id')\n",
    "\n",
    "print(f\">> {title}\")\n",
    "display(df.style.applymap(_df_style_high, subset=df_columns, value_yellow=2, value_red=4, value_greenS=1, value_greenH=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
